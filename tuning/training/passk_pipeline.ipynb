{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dd9be19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 01-31 23:17:25 [__init__.py:244] Automatically detected platform cuda.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6105902/shougan/balance-budget/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/project/6105902/shougan/balance-budget/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from tuning.data.train_dataset import get_train_dataset\n",
    "from tuning.training.config_training import ModelLoadConfig, LoraConfig, SFTRunConfig, PTRunConfig, DPOTrainingConfig, TrainingArgumentsConfig, PassAtKConfig, sft_batch_size, effective_batch_size\n",
    "from tuning.training.perplexity_callback import PerplexityStoppingCallback\n",
    "from tuning.training.passk_callback import PassAtKStoppingCallback\n",
    "from tuning.utils.utils import apply_chat_template, chat_template_func\n",
    "import json\n",
    "import sys\n",
    "from datasets import load_from_disk\n",
    "from typing import List, Optional, Union\n",
    "from pathlib import Path\n",
    "from tuning.config import DATASETS_DIR, HF_MODEL_MAP\n",
    "import os\n",
    "from tuning.training.config_training import DatasetConfig, SFTRunConfig\n",
    "from tuning.config import MODELS_DIR\n",
    "from tuning.training.sft_training import train_model_sft\n",
    "from tuning.training.dpo_training import train_model_dpo\n",
    "import subprocess\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "526586d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tuning.training.passk_callback\n",
    "importlib.reload(tuning.training.passk_callback)\n",
    "from tuning.training.passk_callback import PassAtKStoppingCallback\n",
    "from tuning.training.dpo_training import train_model_dpo\n",
    "importlib.reload(tuning.training.dpo_training)\n",
    "from tuning.training.dpo_training import train_model_dpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "265c0b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"llama3-8B\"\n",
    "total_train_size = 8192  # 29980\n",
    "perplexity_thresholds = [7.0,6.0, 5.75, 5.5, 5.25, 5.0, 4.75, 4.5, 4.25,4.0, 3.9, 3.8, 3.7, 3.6,3.55,3.5,3.45,3.4,3.35,3.3, 3.25, 3.2, 3.15, 3.1]\n",
    "perplexity_thresholds = [6.0, 5.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acd6103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = DatasetConfig(\n",
    "    dataset = \"tuluif\",\n",
    "    dataset_type = \"sft\",\n",
    "    train_size = total_train_size, # 29980\n",
    ")\n",
    "\n",
    "run_config = SFTRunConfig(\n",
    "    dataset_config = dataset_config,\n",
    "    model_name_hf = HF_MODEL_MAP[MODEL],  # Use HuggingFace model name, not local path\n",
    "    model_name = MODEL,  # Base model name for output directory construction\n",
    "    do_training=True,\n",
    "    do_inference=False,\n",
    "    do_evaluation=False,\n",
    ")\n",
    "passk_config = PassAtKConfig( # this is just to dynamically view the pass@1 of ifeval\n",
    "    target_pass_at_k=[0.1, 0.2, 0.3,0.4,0.5,0.6],\n",
    "    k_values=[1],\n",
    "    n_samples=1,\n",
    "    num_prompts=200,\n",
    "    temperature=0.7,\n",
    "    strict=True,\n",
    "    enabled=True,\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig()\n",
    "model_load_config = ModelLoadConfig()\n",
    "model_load_config.max_seq_length = 4096\n",
    "training_args = TrainingArgumentsConfig()\n",
    "training_args.eval_steps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06e8341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshougan\u001b[0m (\u001b[33mshougan-university-of-waterloo\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/project/6105902/shougan/balance-budget/wandb/run-20260131_232831-ryxlxxfu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shougan-university-of-waterloo/tuning/runs/ryxlxxfu' target=\"_blank\">llama3-8B_sft-tuluif-8192</a></strong> to <a href='https://wandb.ai/shougan-university-of-waterloo/tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shougan-university-of-waterloo/tuning' target=\"_blank\">https://wandb.ai/shougan-university-of-waterloo/tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shougan-university-of-waterloo/tuning/runs/ryxlxxfu' target=\"_blank\">https://wandb.ai/shougan-university-of-waterloo/tuning/runs/ryxlxxfu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting train dataset for run config: llama3-8B_sft-tuluif-8192\n",
      "Checking for dataset at /home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/data/datasets/sft-tuluif-8192\n",
      "Dataset already exists at /home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/data/datasets/sft-tuluif-8192\n",
      "Sampled dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'prompt', 'messages', 'constraints'],\n",
      "        num_rows: 8192\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'prompt', 'messages', 'constraints'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "})\n",
      "Example training row: {'id': 'personas_IF_9u3mcrurksv7hypq3xlppyba', 'prompt': 'Compile a detailed summary of the most recent case discussed on the podcast. Include the title of the episode wrapped in double angular brackets, i.e. <<title>>, and quote at least one statement made by the host during the discussion.', 'messages': [{'content': 'You are a helpful assistant who is an expert at responding to prompts by carefully following the given instructions', 'role': 'system'}, {'content': 'Compile a detailed summary of the most recent case discussed on the podcast. Include the title of the episode wrapped in double angular brackets, i.e. <<title>>, and quote at least one statement made by the host during the discussion.', 'role': 'user'}, {'content': \"I'm sorry, but I don't have access to real-time data, including the most recent episodes of podcasts. If you provide me with some details from the episode, I'd be happy to help you compile a summary based on that information.\", 'role': 'assistant'}], 'constraints': ['format:title', 'use quotation']}\n",
      "Example evaluation row: {'id': 'personas_IF_o3dewf33wk895zom2dbfye5u', 'prompt': 'I want to create a podcast where I explore the themes and motifs found in surreal dystopian literature, both mine and others. This podcast will dive deep into the artistic and philosophical underpinnings of this genre, aiming to become a leading voice in literary criticism. Can you guide me on how to effectively launch and grow this podcast? Include at least 3 **bold text** sections such as: **bold text 1**, **bold text 2**, etc., and incorporate a famous quote about dystopian literature.', 'messages': [{'content': 'You are a helpful assistant who is an expert at responding to prompts by carefully following the given instructions', 'role': 'system'}, {'content': 'I want to create a podcast where I explore the themes and motifs found in surreal dystopian literature, both mine and others. This podcast will dive deep into the artistic and philosophical underpinnings of this genre, aiming to become a leading voice in literary criticism. Can you guide me on how to effectively launch and grow this podcast? Include at least 3 **bold text** sections such as: **bold text 1**, **bold text 2**, etc., and incorporate a famous quote about dystopian literature.', 'role': 'user'}, {'content': 'Launching a podcast on surreal dystopian literature is a fascinating endeavor that requires a combination of passion, strategic planning, and thoughtful execution. Here\\'s a guide to help you effectively launch and grow your podcast:\\n\\n**Identify Your Niche and Audience:** Begin by clearly defining what makes your podcast unique in the realm of dystopian literature. Consider focusing on specific sub-genres, authors, or themes that resonate with you. Understanding your target audience will help tailor your content to their interests and needs.\\n\\n**Develop Compelling Content:** Plan your episodes around intriguing themes and motifs found in surreal dystopian literature. Consider including analyses of classic works, like George Orwell\\'s \"1984\" or Aldous Huxley\\'s \"Brave New World,\" alongside contemporary pieces. A quote to inspire your discussions might be Margaret Atwood\\'s reflection: \"A dystopia is a utopia that\\'s gone wrong,\" which can serve as a launching point for exploring the thin line between utopian ideals and dystopian realities.\\n\\n**Leverage Expertise and Connections:** To establish credibility, invite authors, literary critics, and professors as guests to provide diverse perspectives. Your own insights as a writer can create a rich dialogue, blending personal experiences with broader literary critiques.\\n\\n**Invest in Quality Production:** Ensure high-quality audio and editing to maintain a professional standard. Consider investing in a good microphone and editing software. This attention to detail will enhance listener experience and retention.\\n\\n**Engage Your Audience:** Create a community around your podcast by encouraging listener interaction. Use social media platforms to engage with your audience, gather feedback, and promote upcoming episodes. Consider hosting live Q&A sessions or discussions to deepen listener involvement.\\n\\n**Market Strategically:** Utilize SEO techniques to make your podcast more discoverable. Write detailed show notes and transcriptions for each episode, including relevant keywords about the literature discussed. Collaborate with other podcasters or bloggers in the literary space to expand your reach.\\n\\n**Monitor and Adapt:** Regularly review analytics to understand which episodes resonate most with your audience. Be ready to adapt your content strategy based on listener feedback and trends in the literary world.\\n\\nBy focusing on these key areas, you can launch a podcast that not only explores the depths of surreal dystopian literature but also becomes a leading voice in literary criticism. Remember, consistency and authenticity are crucial to building a loyal listener base.', 'role': 'assistant'}], 'constraints': ['use quotation', 'format:number of highlighted sections']}\n",
      "==((====))==  Unsloth 2025.10.1: Fast Llama patching. Transformers: 4.56.1. vLLM: 0.9.2+computecanada.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.189 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7430d43d2a254acca7e2be3e95dd8f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Will map <|im_end|> to EOS = <|end_of_text|>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'prompt', 'messages', 'constraints', 'text'],\n",
      "        num_rows: 8192\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'prompt', 'messages', 'constraints', 'text'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "})\n",
      "{'id': 'personas_IF_9u3mcrurksv7hypq3xlppyba', 'prompt': 'Compile a detailed summary of the most recent case discussed on the podcast. Include the title of the episode wrapped in double angular brackets, i.e. <<title>>, and quote at least one statement made by the host during the discussion.', 'messages': [{'content': 'You are a helpful assistant who is an expert at responding to prompts by carefully following the given instructions', 'role': 'system'}, {'content': 'Compile a detailed summary of the most recent case discussed on the podcast. Include the title of the episode wrapped in double angular brackets, i.e. <<title>>, and quote at least one statement made by the host during the discussion.', 'role': 'user'}, {'content': \"I'm sorry, but I don't have access to real-time data, including the most recent episodes of podcasts. If you provide me with some details from the episode, I'd be happy to help you compile a summary based on that information.\", 'role': 'assistant'}], 'constraints': ['format:title', 'use quotation'], 'text': \"<|im_start|>system\\nYou are a helpful assistant who is an expert at responding to prompts by carefully following the given instructions<|im_end|>\\n<|im_start|>user\\nCompile a detailed summary of the most recent case discussed on the podcast. Include the title of the episode wrapped in double angular brackets, i.e. <<title>>, and quote at least one statement made by the host during the discussion.<|im_end|>\\n<|im_start|>assistant\\nI'm sorry, but I don't have access to real-time data, including the most recent episodes of podcasts. If you provide me with some details from the episode, I'd be happy to help you compile a summary based on that information.<|im_end|>\\n\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.10.1 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PassAtKCallback] Initialized with pass@1 thresholds=[0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
      "[PassAtKCallback] Training will stop at final threshold: 0.1\n",
      "[PassAtKCallback] k_values=[1] (stopping on k=1)\n",
      "[PassAtKCallback] n_samples=1, temperature=0.7, strict=True\n",
      "[PassAtKCallback] IFEval prompts loaded: 541, num_prompts=200\n",
      "[PassAtKCallback] Using vLLM with model save/load (replicating run_inference_ifeval)\n",
      "[SFT] Will stop training when pass@1 >= 0.6\n",
      "[SFT] Checkpoints will be saved at thresholds: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
      "{'output_dir': '/home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/models/llama3-8B_sft-tuluif-8192', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': True, 'do_predict': False, 'eval_strategy': 'steps', 'prediction_loss_only': False, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 16, 'eval_accumulation_steps': 16, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 5e-05, 'weight_decay': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 2, 'max_steps': -1, 'lr_scheduler_type': 'cosine', 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.1, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': '/home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/models/llama3-8B_sft-tuluif-8192/runs/Jan31_23-29-36_kn176', 'logging_strategy': 'steps', 'logging_first_step': False, 'logging_steps': 1, 'logging_nan_inf_filter': True, 'save_strategy': 'no', 'save_steps': 20, 'save_total_limit': 1, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': True, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': 50, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': None, 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_config': {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}, 'parallelism_config': None, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': 'adamw_8bit', 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': 'every_save', 'hub_token': '<HUB_TOKEN>', 'hub_private_repo': None, 'hub_always_push': False, 'hub_revision': None, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'liger_kernel_config': None, 'eval_use_gather_object': False, 'average_tokens_across_devices': False, 'model_init_kwargs': None, 'chat_template_path': None, 'dataset_text_field': 'text', 'dataset_kwargs': None, 'dataset_num_proc': None, 'eos_token': '<EOS_TOKEN>', 'pad_token': '<PAD_TOKEN>', 'max_length': 1024, 'packing': False, 'packing_strategy': 'bfd', 'padding_free': False, 'pad_to_multiple_of': None, 'eval_packing': None, 'completion_only_loss': None, 'assistant_only_loss': False, 'loss_type': 'nll', 'activation_offloading': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 8,192 | Num Epochs = 2 | Total steps = 1,024\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 16\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 16 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 83,886,080 of 8,114,147,328 (1.03% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PassAtKCallback] on_train_begin: model_name=llama3-8B\n",
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='401' max='1024' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 401/1024 49:05 < 1:16:38, 0.14 it/s, Epoch 0.78/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.152800</td>\n",
       "      <td>1.211077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.092500</td>\n",
       "      <td>1.066213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.986800</td>\n",
       "      <td>1.019403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.096300</td>\n",
       "      <td>0.995863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.960400</td>\n",
       "      <td>0.977358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.995300</td>\n",
       "      <td>0.968069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.906300</td>\n",
       "      <td>0.959852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PassAtKCallback] Saving model to /tmp/tmpmu39rs1y...\n",
      "Found HuggingFace hub cache directory: /home/shougan/.cache/huggingface/hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700884b8051143c793520c0e6622b3d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache directory for required files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Copying 4 files from cache to `/tmp/tmpmu39rs1y`: 100%|â–ˆ| 4/4 [00:07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied all 4 files from cache to `/tmp/tmpmu39rs1y`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆ| 4/4 [00:00<00:00, 92182.51\n",
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:35<00:00,  8.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/tmp/tmpmu39rs1y`\n",
      "[PassAtKCallback] Moving training model to CPU...\n",
      "[PassAtKCallback] Loading model with vLLM from /tmp/tmpmu39rs1y...\n",
      "LLM UTILISATION IS 0.8\n",
      "INFO 01-31 23:33:17 [config.py:841] This model supports multiple tasks: {'reward', 'embed', 'classify', 'generate'}. Defaulting to 'generate'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-31 23:33:17 [config.py:1472] Using max model len 131072\n",
      "INFO 01-31 23:33:17 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 01-31 23:33:18 [__init__.py:2662] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 01-31 23:33:24 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 01-31 23:33:26 [core.py:526] Waiting for init message from front-end.\n",
      "INFO 01-31 23:33:26 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/tmp/tmpmu39rs1y', speculative_config=None, tokenizer='/tmp/tmpmu39rs1y', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/tmp/tmpmu39rs1y, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 01-31 23:33:27 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 01-31 23:33:27 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 01-31 23:33:27 [gpu_model_runner.py:1770] Starting to load model /tmp/tmpmu39rs1y...\n",
      "INFO 01-31 23:33:27 [gpu_model_runner.py:1775] Loading model from scratch...\n",
      "INFO 01-31 23:33:27 [cuda.py:284] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  6.33it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.12it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.62it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.51it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.69it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-31 23:33:30 [default_loader.py:272] Loading weights took 2.44 seconds\n",
      "INFO 01-31 23:33:30 [gpu_model_runner.py:1801] Model loading took 15.0006 GiB and 2.786892 seconds\n",
      "INFO 01-31 23:33:40 [backends.py:508] Using cache directory: /home/shougan/.cache/vllm/torch_compile_cache/07b92b014c/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 01-31 23:33:40 [backends.py:519] Dynamo bytecode transform time: 9.79 s\n",
      "INFO 01-31 23:33:44 [backends.py:181] Cache the graph of shape None for later use\n",
      "INFO 01-31 23:34:05 [backends.py:193] Compiling a graph for general shape takes 24.91 s\n",
      "INFO 01-31 23:34:15 [monitor.py:34] torch.compile takes 34.70 s in total\n",
      "INFO 01-31 23:34:16 [gpu_worker.py:232] Available KV cache memory: 35.44 GiB\n",
      "INFO 01-31 23:34:16 [kv_cache_utils.py:716] GPU KV cache size: 290,288 tokens\n",
      "INFO 01-31 23:34:16 [kv_cache_utils.py:720] Maximum concurrency for 131,072 tokens per request: 2.21x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:23<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-31 23:34:40 [gpu_model_runner.py:2326] Graph capturing finished in 24 secs, took 0.70 GiB\n",
      "INFO 01-31 23:34:40 [core.py:172] init engine (profile, create kv cache, warmup model) took 69.93 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Will map <|im_end|> to EOS = <|im_end|>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PassAtKCallback] Generating 200 prompts x 1 samples...\n",
      "INFO 01-31 23:34:42 [chat_utils.py:444] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00577dae4eb24173b4bc7cb97f289c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fecfb435e7646f78dbc1d251a75c3a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W131 23:34:53.330768529 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PassAtKCallback] Moving training model back to GPU...\n",
      "[PassAtKCallback] Evaluating responses...\n",
      "\n",
      "[PassAtKCallback] Step 50, Data Points 800: pass@1=0.2000 (strict, 200 prompts)\n",
      "[PassAtKCallback] Checking threshold: pass@1 >= 0.2000\n",
      "[PassAtKCallback] Saving sweetspot checkpoint to /home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/models/llama3-8B_pass@1-0.20_sft-800\n",
      "Found HuggingFace hub cache directory: /home/shougan/.cache/huggingface/hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22275d172dd4717bfbe9981b1df9c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache directory for required files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Copying 4 files from cache to `/home/shougan/projects/aip-fredashi/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied all 4 files from cache to `/home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/models/llama3-8B_pass@1-0.20_sft-800`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆ| 4/4 [00:00<00:00, 99864.38\n",
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:53<00:00, 13.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/models/llama3-8B_pass@1-0.20_sft-800`\n",
      "[PassAtKCallback] Sweetspot checkpoint saved with metadata at /home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/models_metadata/llama3-8B_passatk-0131_2329.json\n",
      "[PassAtKCallback] Sweetspot threshold 0.2 reached! \n",
      "[PassAtKCallback] Saving model to /tmp/tmpn2ilvkn3...\n",
      "Found HuggingFace hub cache directory: /home/shougan/.cache/huggingface/hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd0d43386ce444882480753a8e9564f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache directory for required files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Copying 4 files from cache to `/tmp/tmpn2ilvkn3`: 100%|â–ˆ| 4/4 [00:07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied all 4 files from cache to `/tmp/tmpn2ilvkn3`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆ| 4/4 [00:00<00:00, 88768.34\n",
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:32<00:00,  8.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/tmp/tmpn2ilvkn3`\n",
      "[PassAtKCallback] Moving training model to CPU...\n",
      "[PassAtKCallback] Loading model with vLLM from /tmp/tmpn2ilvkn3...\n",
      "LLM UTILISATION IS 0.8\n",
      "INFO 01-31 23:39:35 [config.py:841] This model supports multiple tasks: {'reward', 'embed', 'classify', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 01-31 23:39:35 [config.py:1472] Using max model len 131072\n",
      "INFO 01-31 23:39:35 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 01-31 23:39:43 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 01-31 23:39:46 [core.py:526] Waiting for init message from front-end.\n",
      "INFO 01-31 23:39:46 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/tmp/tmpn2ilvkn3', speculative_config=None, tokenizer='/tmp/tmpn2ilvkn3', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/tmp/tmpn2ilvkn3, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 01-31 23:39:47 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 01-31 23:39:47 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 01-31 23:39:48 [gpu_model_runner.py:1770] Starting to load model /tmp/tmpn2ilvkn3...\n",
      "INFO 01-31 23:39:48 [gpu_model_runner.py:1775] Loading model from scratch...\n",
      "INFO 01-31 23:39:48 [cuda.py:284] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  6.93it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:03<00:03,  1.89s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.36s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.27s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:05<00:00,  1.28s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-31 23:39:53 [default_loader.py:272] Loading weights took 5.23 seconds\n",
      "INFO 01-31 23:39:54 [gpu_model_runner.py:1801] Model loading took 15.0006 GiB and 5.543589 seconds\n",
      "INFO 01-31 23:40:04 [backends.py:508] Using cache directory: /home/shougan/.cache/vllm/torch_compile_cache/c585887047/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 01-31 23:40:04 [backends.py:519] Dynamo bytecode transform time: 10.23 s\n",
      "INFO 01-31 23:40:08 [backends.py:181] Cache the graph of shape None for later use\n",
      "INFO 01-31 23:40:29 [backends.py:193] Compiling a graph for general shape takes 24.20 s\n",
      "INFO 01-31 23:40:38 [monitor.py:34] torch.compile takes 34.44 s in total\n",
      "INFO 01-31 23:40:39 [gpu_worker.py:232] Available KV cache memory: 35.44 GiB\n",
      "INFO 01-31 23:40:39 [kv_cache_utils.py:716] GPU KV cache size: 290,288 tokens\n",
      "INFO 01-31 23:40:39 [kv_cache_utils.py:720] Maximum concurrency for 131,072 tokens per request: 2.21x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:20<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-31 23:41:00 [gpu_model_runner.py:2326] Graph capturing finished in 20 secs, took 0.70 GiB\n",
      "INFO 01-31 23:41:00 [core.py:172] init engine (profile, create kv cache, warmup model) took 66.16 seconds\n",
      "[PassAtKCallback] Generating 200 prompts x 1 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc89009d52464873864d65add7ed097c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba1af87e3d66450bad4684ca4afc41e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W131 23:41:13.204180722 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PassAtKCallback] Moving training model back to GPU...\n",
      "[PassAtKCallback] Evaluating responses...\n",
      "\n",
      "[PassAtKCallback] Step 100, Data Points 1600: pass@1=0.3750 (strict, 200 prompts)\n",
      "[PassAtKCallback] Checking threshold: pass@1 >= 0.3000\n",
      "[PassAtKCallback] Saving sweetspot checkpoint to /home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/models/llama3-8B_pass@1-0.30_sft-1600\n",
      "Found HuggingFace hub cache directory: /home/shougan/.cache/huggingface/hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "311d6658b2764892a6fe7c4f27b116e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache directory for required files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Copying 4 files from cache to `/home/shougan/projects/aip-fredashi/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied all 4 files from cache to `/home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/models/llama3-8B_pass@1-0.30_sft-1600`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆ| 4/4 [00:00<00:00, 100462.3\n",
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:50<00:00, 12.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/models/llama3-8B_pass@1-0.30_sft-1600`\n",
      "[PassAtKCallback] Sweetspot checkpoint saved with metadata at /home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/models_metadata/llama3-8B_passatk-0131_2329.json\n",
      "[PassAtKCallback] Sweetspot threshold 0.3 reached! \n",
      "[PassAtKCallback] Saving model to /tmp/tmpml7p54qf...\n",
      "Found HuggingFace hub cache directory: /home/shougan/.cache/huggingface/hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773b85c84520449ebc46b5bb50b7e35a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache directory for required files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Copying 4 files from cache to `/tmp/tmpml7p54qf`: 100%|â–ˆ| 4/4 [00:07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied all 4 files from cache to `/tmp/tmpml7p54qf`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆ| 4/4 [00:00<00:00, 96978.13\n",
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:33<00:00,  8.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/tmp/tmpml7p54qf`\n",
      "[PassAtKCallback] Moving training model to CPU...\n",
      "[PassAtKCallback] Loading model with vLLM from /tmp/tmpml7p54qf...\n",
      "LLM UTILISATION IS 0.8\n",
      "INFO 01-31 23:45:53 [config.py:841] This model supports multiple tasks: {'reward', 'embed', 'classify', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 01-31 23:45:53 [config.py:1472] Using max model len 131072\n",
      "INFO 01-31 23:45:53 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 01-31 23:46:10 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 01-31 23:46:13 [core.py:526] Waiting for init message from front-end.\n",
      "INFO 01-31 23:46:13 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/tmp/tmpml7p54qf', speculative_config=None, tokenizer='/tmp/tmpml7p54qf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/tmp/tmpml7p54qf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 01-31 23:46:15 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 01-31 23:46:15 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 01-31 23:46:15 [gpu_model_runner.py:1770] Starting to load model /tmp/tmpml7p54qf...\n",
      "INFO 01-31 23:46:15 [gpu_model_runner.py:1775] Loading model from scratch...\n",
      "INFO 01-31 23:46:15 [cuda.py:284] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  3.34it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:02,  1.06s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.06it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.09it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.12it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-31 23:46:19 [default_loader.py:272] Loading weights took 3.65 seconds\n",
      "INFO 01-31 23:46:20 [gpu_model_runner.py:1801] Model loading took 15.0006 GiB and 4.050285 seconds\n",
      "INFO 01-31 23:46:29 [backends.py:508] Using cache directory: /home/shougan/.cache/vllm/torch_compile_cache/3198971f21/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 01-31 23:46:29 [backends.py:519] Dynamo bytecode transform time: 9.56 s\n",
      "INFO 01-31 23:46:33 [backends.py:181] Cache the graph of shape None for later use\n",
      "INFO 01-31 23:46:53 [backends.py:193] Compiling a graph for general shape takes 23.78 s\n",
      "INFO 01-31 23:47:02 [monitor.py:34] torch.compile takes 33.34 s in total\n",
      "INFO 01-31 23:47:03 [gpu_worker.py:232] Available KV cache memory: 35.44 GiB\n",
      "INFO 01-31 23:47:04 [kv_cache_utils.py:716] GPU KV cache size: 290,288 tokens\n",
      "INFO 01-31 23:47:04 [kv_cache_utils.py:720] Maximum concurrency for 131,072 tokens per request: 2.21x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:26<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-31 23:47:31 [gpu_model_runner.py:2326] Graph capturing finished in 27 secs, took 0.70 GiB\n",
      "INFO 01-31 23:47:31 [core.py:172] init engine (profile, create kv cache, warmup model) took 71.50 seconds\n",
      "[PassAtKCallback] Generating 200 prompts x 1 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a148d823b8454b53bf31de1092a21da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5075fcf94cf7492d8c1ae0bac3352f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W131 23:47:44.439165654 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PassAtKCallback] Moving training model back to GPU...\n",
      "[PassAtKCallback] Evaluating responses...\n",
      "\n",
      "[PassAtKCallback] Step 150, Data Points 2400: pass@1=0.4700 (strict, 200 prompts)\n",
      "[PassAtKCallback] Checking threshold: pass@1 >= 0.4000\n",
      "[PassAtKCallback] Saving sweetspot checkpoint to /home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/models/llama3-8B_pass@1-0.40_sft-2400\n",
      "Found HuggingFace hub cache directory: /home/shougan/.cache/huggingface/hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a589a1df98b4a87a9c2fb9f04b11c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache directory for required files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Copying 4 files from cache to `/home/shougan/projects/aip-fredashi/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied all 4 files from cache to `/home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/models/llama3-8B_pass@1-0.40_sft-2400`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆ| 4/4 [00:00<00:00, 82241.25\n",
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:52<00:00, 13.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/models/llama3-8B_pass@1-0.40_sft-2400`\n",
      "[PassAtKCallback] Sweetspot checkpoint saved with metadata at /home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/models_metadata/llama3-8B_passatk-0131_2329.json\n",
      "[PassAtKCallback] Sweetspot threshold 0.4 reached! \n",
      "[PassAtKCallback] Saving model to /tmp/tmp_xsc7kd4...\n",
      "Found HuggingFace hub cache directory: /home/shougan/.cache/huggingface/hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20340006f38b400ea21f6a688901b5a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache directory for required files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Copying 4 files from cache to `/tmp/tmp_xsc7kd4`: 100%|â–ˆ| 4/4 [00:08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied all 4 files from cache to `/tmp/tmp_xsc7kd4`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆ| 4/4 [00:00<00:00, 79512.87\n",
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:34<00:00,  8.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/tmp/tmp_xsc7kd4`\n",
      "[PassAtKCallback] Moving training model to CPU...\n",
      "[PassAtKCallback] Loading model with vLLM from /tmp/tmp_xsc7kd4...\n",
      "LLM UTILISATION IS 0.8\n",
      "INFO 01-31 23:53:24 [config.py:841] This model supports multiple tasks: {'reward', 'embed', 'classify', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 01-31 23:53:24 [config.py:1472] Using max model len 131072\n",
      "INFO 01-31 23:53:24 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 01-31 23:53:41 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 01-31 23:53:44 [core.py:526] Waiting for init message from front-end.\n",
      "INFO 01-31 23:53:44 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/tmp/tmp_xsc7kd4', speculative_config=None, tokenizer='/tmp/tmp_xsc7kd4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/tmp/tmp_xsc7kd4, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 01-31 23:53:46 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 01-31 23:53:46 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 01-31 23:53:46 [gpu_model_runner.py:1770] Starting to load model /tmp/tmp_xsc7kd4...\n",
      "INFO 01-31 23:53:46 [gpu_model_runner.py:1775] Loading model from scratch...\n",
      "INFO 01-31 23:53:46 [cuda.py:284] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  6.65it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.74it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.28it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.26it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.40it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-31 23:53:49 [default_loader.py:272] Loading weights took 3.00 seconds\n",
      "INFO 01-31 23:53:50 [gpu_model_runner.py:1801] Model loading took 15.0006 GiB and 3.324723 seconds\n",
      "INFO 01-31 23:53:59 [backends.py:508] Using cache directory: /home/shougan/.cache/vllm/torch_compile_cache/0598dda45b/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 01-31 23:53:59 [backends.py:519] Dynamo bytecode transform time: 9.50 s\n",
      "INFO 01-31 23:54:03 [backends.py:181] Cache the graph of shape None for later use\n",
      "INFO 01-31 23:54:24 [backends.py:193] Compiling a graph for general shape takes 24.61 s\n",
      "INFO 01-31 23:54:33 [monitor.py:34] torch.compile takes 34.11 s in total\n",
      "INFO 01-31 23:54:34 [gpu_worker.py:232] Available KV cache memory: 35.44 GiB\n",
      "INFO 01-31 23:54:35 [kv_cache_utils.py:716] GPU KV cache size: 290,288 tokens\n",
      "INFO 01-31 23:54:35 [kv_cache_utils.py:720] Maximum concurrency for 131,072 tokens per request: 2.21x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:21<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-31 23:54:58 [gpu_model_runner.py:2326] Graph capturing finished in 22 secs, took 0.70 GiB\n",
      "INFO 01-31 23:54:58 [core.py:172] init engine (profile, create kv cache, warmup model) took 68.13 seconds\n",
      "[PassAtKCallback] Generating 200 prompts x 1 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082f4ce6ec324f11901f3e2b63ad062d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93838fb7f79a4236be54c4ba7df6b2d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W131 23:55:11.370670492 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PassAtKCallback] Moving training model back to GPU...\n",
      "[PassAtKCallback] Evaluating responses...\n",
      "\n",
      "[PassAtKCallback] Step 200, Data Points 3200: pass@1=0.4750 (strict, 200 prompts)\n",
      "[PassAtKCallback] Checking threshold: pass@1 >= 0.1000\n",
      "[PassAtKCallback] Saving sweetspot checkpoint to /home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/models/llama3-8B_pass@1-0.10_sft-3200\n",
      "Found HuggingFace hub cache directory: /home/shougan/.cache/huggingface/hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625cbdfabccd46aba2b04360f9aa4c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache directory for required files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Copying 4 files from cache to `/home/shougan/projects/aip-fredashi/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied all 4 files from cache to `/home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/models/llama3-8B_pass@1-0.10_sft-3200`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆ| 4/4 [00:00<00:00, 101067.5\n",
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:50<00:00, 12.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/models/llama3-8B_pass@1-0.10_sft-3200`\n",
      "[PassAtKCallback] Sweetspot checkpoint saved with metadata at /home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/models_metadata/llama3-8B_passatk-0131_2329.json\n",
      "[PassAtKCallback] Sweetspot threshold 0.1 reached! \n",
      "[PassAtKCallback] Saving model to /tmp/tmp7z6g4n9y...\n",
      "Found HuggingFace hub cache directory: /home/shougan/.cache/huggingface/hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c5d32bf7a5c4fe0a418c24f3b483eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache directory for required files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Copying 4 files from cache to `/tmp/tmp7z6g4n9y`: 100%|â–ˆ| 4/4 [00:09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied all 4 files from cache to `/tmp/tmp7z6g4n9y`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆ| 4/4 [00:00<00:00, 96978.13\n",
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:40<00:00, 10.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/tmp/tmp7z6g4n9y`\n",
      "[PassAtKCallback] Moving training model to CPU...\n",
      "[PassAtKCallback] Loading model with vLLM from /tmp/tmp7z6g4n9y...\n",
      "LLM UTILISATION IS 0.8\n",
      "INFO 02-01 00:00:50 [config.py:841] This model supports multiple tasks: {'reward', 'embed', 'classify', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 02-01 00:00:50 [config.py:1472] Using max model len 131072\n",
      "INFO 02-01 00:00:50 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 02-01 00:01:08 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 02-01 00:01:11 [core.py:526] Waiting for init message from front-end.\n",
      "INFO 02-01 00:01:11 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/tmp/tmp7z6g4n9y', speculative_config=None, tokenizer='/tmp/tmp7z6g4n9y', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/tmp/tmp7z6g4n9y, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 02-01 00:01:13 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 02-01 00:01:13 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 02-01 00:01:13 [gpu_model_runner.py:1770] Starting to load model /tmp/tmp7z6g4n9y...\n",
      "INFO 02-01 00:01:13 [gpu_model_runner.py:1775] Loading model from scratch...\n",
      "INFO 02-01 00:01:13 [cuda.py:284] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.99it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.85it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.59it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.45s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.11s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-01 00:01:18 [default_loader.py:272] Loading weights took 4.55 seconds\n",
      "INFO 02-01 00:01:18 [gpu_model_runner.py:1801] Model loading took 15.0006 GiB and 4.953219 seconds\n",
      "INFO 02-01 00:01:29 [backends.py:508] Using cache directory: /home/shougan/.cache/vllm/torch_compile_cache/e06b3c7211/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 02-01 00:01:29 [backends.py:519] Dynamo bytecode transform time: 10.11 s\n",
      "INFO 02-01 00:01:32 [backends.py:181] Cache the graph of shape None for later use\n",
      "INFO 02-01 00:01:53 [backends.py:193] Compiling a graph for general shape takes 24.16 s\n",
      "INFO 02-01 00:02:02 [monitor.py:34] torch.compile takes 34.28 s in total\n",
      "INFO 02-01 00:02:03 [gpu_worker.py:232] Available KV cache memory: 35.44 GiB\n",
      "INFO 02-01 00:02:03 [kv_cache_utils.py:716] GPU KV cache size: 290,288 tokens\n",
      "INFO 02-01 00:02:03 [kv_cache_utils.py:720] Maximum concurrency for 131,072 tokens per request: 2.21x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:21<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-01 00:02:25 [gpu_model_runner.py:2326] Graph capturing finished in 22 secs, took 0.70 GiB\n",
      "INFO 02-01 00:02:26 [core.py:172] init engine (profile, create kv cache, warmup model) took 67.29 seconds\n",
      "[PassAtKCallback] Generating 200 prompts x 1 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48013a58272e4757b1081aba9e25cf3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a705e50a22c9463db8158bea5f0c0b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W201 00:02:39.432693382 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PassAtKCallback] Moving training model back to GPU...\n",
      "[PassAtKCallback] Evaluating responses...\n",
      "\n",
      "[PassAtKCallback] Step 250, Data Points 4000: pass@1=0.5150 (strict, 200 prompts)\n",
      "[PassAtKCallback] Checking threshold: pass@1 >= 0.5000\n",
      "[PassAtKCallback] Saving sweetspot checkpoint to /home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/models/llama3-8B_pass@1-0.50_sft-4000\n",
      "Found HuggingFace hub cache directory: /home/shougan/.cache/huggingface/hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ced76ea9f54d289dff2a2e09fb4241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache directory for required files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Copying 4 files from cache to `/home/shougan/projects/aip-fredashi/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied all 4 files from cache to `/home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/models/llama3-8B_pass@1-0.50_sft-4000`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆ| 4/4 [00:00<00:00, 94254.02\n",
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:51<00:00, 12.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/models/llama3-8B_pass@1-0.50_sft-4000`\n",
      "[PassAtKCallback] Sweetspot checkpoint saved with metadata at /home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/models_metadata/llama3-8B_passatk-0131_2329.json\n",
      "[PassAtKCallback] Sweetspot threshold 0.5 reached! \n",
      "[PassAtKCallback] Saving model to /tmp/tmpzzdvqxqj...\n",
      "Found HuggingFace hub cache directory: /home/shougan/.cache/huggingface/hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0326231827e4355b0bc25ffaa18abb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache directory for required files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Copying 4 files from cache to `/tmp/tmpzzdvqxqj`: 100%|â–ˆ| 4/4 [00:07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied all 4 files from cache to `/tmp/tmpzzdvqxqj`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆ| 4/4 [00:00<00:00, 101067.5\n",
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:32<00:00,  8.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/tmp/tmpzzdvqxqj`\n",
      "[PassAtKCallback] Moving training model to CPU...\n",
      "[PassAtKCallback] Loading model with vLLM from /tmp/tmpzzdvqxqj...\n",
      "LLM UTILISATION IS 0.8\n",
      "INFO 02-01 00:08:33 [config.py:841] This model supports multiple tasks: {'reward', 'embed', 'classify', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 02-01 00:08:33 [config.py:1472] Using max model len 131072\n",
      "INFO 02-01 00:08:33 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 02-01 00:08:50 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 02-01 00:08:53 [core.py:526] Waiting for init message from front-end.\n",
      "INFO 02-01 00:08:53 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/tmp/tmpzzdvqxqj', speculative_config=None, tokenizer='/tmp/tmpzzdvqxqj', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/tmp/tmpzzdvqxqj, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 02-01 00:08:54 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 02-01 00:08:54 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 02-01 00:08:54 [gpu_model_runner.py:1770] Starting to load model /tmp/tmpzzdvqxqj...\n",
      "INFO 02-01 00:08:54 [gpu_model_runner.py:1775] Loading model from scratch...\n",
      "INFO 02-01 00:08:55 [cuda.py:284] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  6.24it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.23it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.72it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.55it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.74it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-01 00:08:57 [default_loader.py:272] Loading weights took 2.41 seconds\n",
      "INFO 02-01 00:08:58 [gpu_model_runner.py:1801] Model loading took 15.0006 GiB and 2.723075 seconds\n",
      "INFO 02-01 00:09:08 [backends.py:508] Using cache directory: /home/shougan/.cache/vllm/torch_compile_cache/d75979b04e/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 02-01 00:09:08 [backends.py:519] Dynamo bytecode transform time: 9.84 s\n",
      "INFO 02-01 00:09:11 [backends.py:181] Cache the graph of shape None for later use\n",
      "INFO 02-01 00:09:32 [backends.py:193] Compiling a graph for general shape takes 23.86 s\n",
      "INFO 02-01 00:09:41 [monitor.py:34] torch.compile takes 33.70 s in total\n",
      "INFO 02-01 00:09:41 [gpu_worker.py:232] Available KV cache memory: 35.44 GiB\n",
      "INFO 02-01 00:09:42 [kv_cache_utils.py:716] GPU KV cache size: 290,288 tokens\n",
      "INFO 02-01 00:09:42 [kv_cache_utils.py:720] Maximum concurrency for 131,072 tokens per request: 2.21x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:21<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-01 00:10:03 [gpu_model_runner.py:2326] Graph capturing finished in 21 secs, took 0.70 GiB\n",
      "INFO 02-01 00:10:03 [core.py:172] init engine (profile, create kv cache, warmup model) took 65.79 seconds\n",
      "[PassAtKCallback] Generating 200 prompts x 1 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19d6ce2e9644befbc00a290d2af4213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bac431a57ac4ab1b76899bab546cb47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W201 00:10:17.148494731 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PassAtKCallback] Moving training model back to GPU...\n",
      "[PassAtKCallback] Evaluating responses...\n",
      "\n",
      "[PassAtKCallback] Step 300, Data Points 4800: pass@1=0.5400 (strict, 200 prompts)\n",
      "[PassAtKCallback] Saving model to /tmp/tmp2osmxynh...\n",
      "Found HuggingFace hub cache directory: /home/shougan/.cache/huggingface/hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1dfadbefa17421f9d4babde95503fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache directory for required files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Copying 4 files from cache to `/tmp/tmp2osmxynh`: 100%|â–ˆ| 4/4 [00:57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied all 4 files from cache to `/tmp/tmp2osmxynh`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆ| 4/4 [00:00<00:00, 77672.30\n",
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:30<00:00,  7.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/tmp/tmp2osmxynh`\n",
      "[PassAtKCallback] Moving training model to CPU...\n",
      "[PassAtKCallback] Loading model with vLLM from /tmp/tmp2osmxynh...\n",
      "LLM UTILISATION IS 0.8\n",
      "INFO 02-01 00:14:36 [config.py:841] This model supports multiple tasks: {'reward', 'embed', 'classify', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 02-01 00:14:36 [config.py:1472] Using max model len 131072\n",
      "INFO 02-01 00:14:36 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 02-01 00:14:50 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 02-01 00:14:53 [core.py:526] Waiting for init message from front-end.\n",
      "INFO 02-01 00:14:53 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/tmp/tmp2osmxynh', speculative_config=None, tokenizer='/tmp/tmp2osmxynh', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/tmp/tmp2osmxynh, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 02-01 00:14:55 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 02-01 00:14:55 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 02-01 00:14:55 [gpu_model_runner.py:1770] Starting to load model /tmp/tmp2osmxynh...\n",
      "INFO 02-01 00:14:55 [gpu_model_runner.py:1775] Loading model from scratch...\n",
      "INFO 02-01 00:14:55 [cuda.py:284] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.13it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.66it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.48it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.12s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.08it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-01 00:14:59 [default_loader.py:272] Loading weights took 3.81 seconds\n",
      "INFO 02-01 00:15:00 [gpu_model_runner.py:1801] Model loading took 15.0006 GiB and 4.153231 seconds\n",
      "INFO 02-01 00:15:09 [backends.py:508] Using cache directory: /home/shougan/.cache/vllm/torch_compile_cache/68e51802a5/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 02-01 00:15:09 [backends.py:519] Dynamo bytecode transform time: 9.58 s\n",
      "INFO 02-01 00:15:13 [backends.py:181] Cache the graph of shape None for later use\n",
      "INFO 02-01 00:15:34 [backends.py:193] Compiling a graph for general shape takes 24.02 s\n",
      "INFO 02-01 00:15:43 [monitor.py:34] torch.compile takes 33.60 s in total\n",
      "INFO 02-01 00:15:43 [gpu_worker.py:232] Available KV cache memory: 35.44 GiB\n",
      "INFO 02-01 00:15:44 [kv_cache_utils.py:716] GPU KV cache size: 290,288 tokens\n",
      "INFO 02-01 00:15:44 [kv_cache_utils.py:720] Maximum concurrency for 131,072 tokens per request: 2.21x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:21<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-01 00:16:06 [gpu_model_runner.py:2326] Graph capturing finished in 22 secs, took 0.70 GiB\n",
      "INFO 02-01 00:16:06 [core.py:172] init engine (profile, create kv cache, warmup model) took 66.11 seconds\n",
      "[PassAtKCallback] Generating 200 prompts x 1 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775bdc8c2fb04c0aa9e536cecb1a2b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01703272cc14f18af89b1106a528748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W201 00:16:19.496690910 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PassAtKCallback] Moving training model back to GPU...\n",
      "[PassAtKCallback] Evaluating responses...\n",
      "\n",
      "[PassAtKCallback] Step 350, Data Points 5600: pass@1=0.5750 (strict, 200 prompts)\n",
      "[PassAtKCallback] Saving model to /tmp/tmp2c8yoola...\n",
      "Found HuggingFace hub cache directory: /home/shougan/.cache/huggingface/hub\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c54d8a7d6e462e981a5a4f7a181685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache directory for required files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Copying 4 files from cache to `/tmp/tmp2c8yoola`: 100%|â–ˆ| 4/4 [00:22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied all 4 files from cache to `/tmp/tmp2c8yoola`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆ| 4/4 [00:00<00:00, 93206.76\n",
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:30<00:00,  7.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/tmp/tmp2c8yoola`\n",
      "[PassAtKCallback] Moving training model to CPU...\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "run = wandb.init(\n",
    "    name=run_config.run_name, \n",
    "    project=\"tuning\", \n",
    "    reinit=True,\n",
    "    # Optional: Pass config here so it's logged even if training crashes early\n",
    "    config=run_config.__dict__ if hasattr(run_config, \"__dict__\") else {} \n",
    ")\n",
    "\n",
    "with run:\n",
    "    model, tokenizer, trainer, callbacks = train_model_sft(\n",
    "        run_config = run_config,\n",
    "        lora_config = lora_config,\n",
    "        model_load_config = model_load_config,\n",
    "        training_args = training_args,\n",
    "        passk_config = passk_config\n",
    "    )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c57f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'threshold_type': 'perplexity', 'threshold_value': 6.0, 'global_step': 40, 'checkpoint_path': '/home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/models/llama3-8B_ppl-6.00_sft-640', 'data_points_seen': 640}]\n"
     ]
    }
   ],
   "source": [
    "ppl_callback = callbacks[-1]\n",
    "metadata_file = ppl_callback.metadata_path\n",
    "checkpoints = []\n",
    "with open(metadata_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        checkpoints.append(json.loads(line))\n",
    "print(checkpoints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a4c83a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m torch.cuda.empty_cache()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mmodel\u001b[49m, tokenizer, trainer\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "del model, tokenizer, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c940a3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per device train batch size: 1\n",
      "Getting train dataset for run config: llama3-8B_llama3-8B_ppl-6.00_sft-640_pt-tuluif-7552\n",
      "Checking for dataset at /home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/data/datasets/pt-tuluif-7552\n",
      "Dataset already exists at /home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/data/datasets/pt-tuluif-7552\n",
      "Sampled dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'prompt', 'constraints', 'chosen', 'rejected', 'chonsen_model', 'rejected_model', 'system_message'],\n",
      "        num_rows: 7552\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'prompt', 'constraints', 'chosen', 'rejected', 'chonsen_model', 'rejected_model', 'system_message'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "})\n",
      "Example training row: {'id': 'personas_IF_dqxglsux2n8jeu59qktlnesh', 'prompt': 'Name two famous equestrian events that are part of the international jumping circuit, and format your answer by choosing one from these options: lowercase, UPPERCASE, Title Case.', 'constraints': ['punctuation:use no comma', 'format:choose one from options'], 'chosen': 'LONGINES GLOBAL CHAMPIONS TOUR, ROLEX GRAND SLAM OF SHOW JUMPING', 'rejected': '- longines global champions tour\\n- FEI WORLD CUP JUMPING', 'chonsen_model': 'gpt-4o', 'rejected_model': 'gpt-4o', 'system_message': 'You are a helpful assistant who is an expert at responding to prompts by carefully following the given instructions'}\n",
      "Example evaluation row: {'id': 'personas_IF_kvn9ms052ydm1n10j2zs7c90', 'prompt': 'As a high school student with a passion for astronomy, create a list of reliable sources to learn about the solar system. Each source description should contain the keywords \"planet\" and \"orbit\" at least twice. Provide two separate lists: one for online resources and another for books. Each list should contain exactly 5 bullet points.', 'constraints': ['keywords:letter frequency', ' give two responses', 'format:number of bullet lists'], 'chosen': '**Online Resources:**\\n\\n1. **NASA\\'s Solar System Exploration Website**  \\n   This comprehensive online resource offers detailed information about each planet in our solar system and their unique orbits. With interactive features, you can explore how each planet\\'s orbit affects its climate and visibility.\\n\\n2. **The Planetary Society\\'s Guide to the Solar System**  \\n   This guide provides insights into the planets and their orbits, allowing users to understand the dynamic nature of planetary orbits. Learn how each planet\\'s orbit influences its interaction with other celestial bodies.\\n\\n3. **National Geographic\\'s Solar System 101**  \\n   An engaging online article that covers the basics of each planet, including their orbits around the Sun. Discover how the orbit of each planet contributes to phenomena like seasons and eclipses.\\n\\n4. **BBC\\'s Planets and Orbits Webpage**  \\n   This webpage is part of the BBC\\'s educational content, focusing on the planets and their orbits. It explains the intricacies of how each planet maintains its orbit and the historical context of orbit discoveries.\\n\\n5. **Khan Academy\\'s Astronomy Course**  \\n   This free, educational platform offers a series of videos on the solar system, emphasizing the planets and their orbits. Learn about the gravitational forces that govern each planet\\'s orbit and their effects on the solar system\\'s stability.\\n\\n**Books:**\\n\\n1. **\"The Planets: An Illustrated History of Our Solar System\" by Carole Stott**  \\n   This book provides an in-depth look at each planet, with a special focus on their orbits. It explains how the orbit of each planet was discovered and how it affects the solar system.\\n\\n2. **\"The Solar System: An Astronomer\\'s Guide\" by David A. Rothery**  \\n   This guide explores the planets and their orbits, highlighting the science behind orbital mechanics. Learn how each planet\\'s orbit is influenced by gravitational forces.\\n\\n3. **\"Planetary Science: The Science of Planets around Stars\" by George H. A. Cole and Michael M. Woolfson**  \\n   This textbook covers the planets in our solar system and the orbits they follow. It provides a detailed analysis of how each planet\\'s orbit affects its environment and evolution.\\n\\n4. **\"The Orbits of Planets: A Journey Through the Solar System\" by John E. Chambers**  \\n   Focused on the paths that planets take, this book explores the orbits of each planet. It delves into the history and science of how planetary orbits were understood.\\n\\n5. **\"Planets and Life: The Emerging Science of Astrobiology\" by Woodruff T. Sullivan III and John Baross**  \\n   This book examines the planets and their orbits, discussing how orbital patterns can influence the potential for life. Understand how each planet\\'s orbit affects its climate and habitability.', 'rejected': '### Online Resources\\n\\n- **NASA Solar System Exploration**: This official NASA website provides comprehensive information about each planet in our solar system, detailing their orbits, characteristics, and exploration missions. It includes interactive features to visualize the orbits of planets and their positions in the solar system, making it an excellent resource for understanding planetary orbits.\\n\\n- **National Geographic\\'s Solar System 101**: An informative section on National Geographic\\'s website that covers the basics of each planet, their unique features, and how they orbit the Sun. With rich visual content, it helps users grasp the concept of planetary orbits and how each planet\\'s orbit affects its climate and environment.\\n\\n- **European Space Agency (ESA) Solar System and Beyond**: The ESA website offers detailed articles and videos about the planets and their orbits, focusing on the latest scientific discoveries. It explains how the orbits of planets are studied using various space missions and the impact of these orbits on solar system dynamics.\\n\\n- **BBC\\'s The Planets**: This online series explores the planets of our solar system, emphasizing their orbits and how they have evolved over time. It features engaging multimedia content that discusses the unique orbital paths of each planet and how these orbits contribute to their characteristics.\\n\\n- **Space.com Solar System Guide**: Space.com provides articles and infographics about the planets and their orbits, offering a user-friendly approach to learning about the solar system. The guide explains the significance of a planet\\'s orbit, how it is calculated, and its relevance to astronomical studies.\\n\\n### Books\\n\\n- **\"The Planets\" by Dava Sobel**: This book offers a captivating journey through the solar system, exploring each planet and its orbit in detail. It delves into the history of how the orbits of planets were discovered and the impact of these orbits on our understanding of the cosmos.\\n\\n- **\"National Geographic Kids: First Big Book of Space\" by Catherine D. Hughes**: Aimed at younger readers, this book introduces the planets and their orbits with colorful illustrations and simple explanations. It highlights how each planet\\'s orbit affects its position in the solar system and its relationship with other planets.\\n\\n- **\"The Solar System: A Visual Exploration of the Planets, Moons, and Other Heavenly Bodies that Orbit Our Sun\" by Marcus Chown**: This visually stunning book provides in-depth information about the planets and their orbits. It discusses the science behind planetary orbits and how these orbits influence the solar system\\'s structure.\\n\\n- **\"Astronomy for Kids: How to Explore Outer Space with Binoculars, a Telescope, or Just Your Eyes!\" by Bruce Betts**: This educational book encourages young astronomers to observe the planets and their orbits. It explains how to track the orbits of planets using simple tools and understand their movement across the sky.\\n\\n- **\"Planetarium\" by Raman Prinja**: Part of the Welcome to the Museum series, this book offers a detailed look at the planets and their orbits. It includes illustrations and explanations of how the orbits of planets are studied and why these orbits are crucial for understanding the solar system\\'s mechanics.', 'chonsen_model': 'gpt-4o', 'rejected_model': 'gpt-4o', 'system_message': 'You are a helpful assistant who is an expert at responding to prompts by carefully following the given instructions'}\n",
      "Loading model from /home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/models/llama3-8B_ppl-6.00_sft-640\n",
      "==((====))==  Unsloth 2025.10.1: Fast Llama patching. Transformers: 4.56.1. vLLM: 0.9.2+computecanada.\n",
      "   \\\\   /|    NVIDIA L40S. Num GPUs = 1. Max memory: 44.403 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4595c428a1074d519b6cf5fac5c79540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for checkpoint in checkpoints:\n",
    "    model_name = Path(checkpoint[\"checkpoint_path\"]).name\n",
    "    data = total_train_size - checkpoint[\"data_points_seen\"] \n",
    "    model_load_config = ModelLoadConfig()\n",
    "    training_args = DPOTrainingConfig()\n",
    "    training_args.eval_steps = 5\n",
    "    dataset_config = DatasetConfig(\n",
    "        dataset = \"tuluif\",\n",
    "        dataset_type = \"pt\",\n",
    "        train_size = data,\n",
    "    )\n",
    "    sft_run_config = SFTRunConfig(\n",
    "        dataset_config = DatasetConfig(\n",
    "            dataset = \"tuluif\",\n",
    "            dataset_type = \"sft\",\n",
    "            train_size = checkpoint[\"data_points_seen\"],\n",
    "            dynamic_path = model_name\n",
    "        ),\n",
    "        model_name = MODEL,\n",
    "        model_name_hf = HF_MODEL_MAP[MODEL], \n",
    "        task_name = \"ifeval\"\n",
    "    )\n",
    "    run_config = PTRunConfig(\n",
    "        dataset_config = dataset_config,\n",
    "        # model_name_hf = HF_MODEL_MAP[MODEL],  \n",
    "        model_name = MODEL,  \n",
    "        sft_run_config = sft_run_config,\n",
    "        task_name = \"ifeval\",\n",
    "        pft_method = \"dpo\",\n",
    "        do_training = True\n",
    "    )\n",
    "    passk_config = PassAtKConfig( # this is just to dynamically view the pass@1 of ifeval\n",
    "        target_pass_at_k=[1.2],\n",
    "        k_values=[1],\n",
    "        n_samples=1,\n",
    "        num_prompts=50,\n",
    "        temperature=0.7,\n",
    "        strict=True,\n",
    "        enabled=True,\n",
    "    )\n",
    "    train_model_dpo(\n",
    "        run_config = run_config,\n",
    "        lora_config = lora_config,\n",
    "        model_load_config = model_load_config,\n",
    "        training_args = training_args,\n",
    "        passk_config = passk_config,\n",
    "        perplexity_thresholds= [0.1] # dummy value to periodically check perplexities too\n",
    "    )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1616e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: python tuning/run_dpo.py --model 'llama3-8B' --dataset 'tuluif' --train_size '7552' --task 'ifeval' --pft_method 'dpo' --do_training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/6105902/shougan/balance-budget/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/project/6105902/shougan/balance-budget/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning\n",
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 01-25 20:38:02 [__init__.py:244] Automatically detected platform cuda.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "{'model': 'llama3-8B', 'dataset': 'tuluif', 'train_size': 7552, 'task': 'ifeval', 'pft_method': 'dpo', 'dynamic_path': 'llama3-8B_ppl-6.00_sft-640', 'sft_train_size': 640, 'do_training': True, 'do_inference': False, 'do_evaluation': False}\n",
      "pt-tuluif-7552\n",
      "llama3-8B_llama3-8B_ppl-6.00_sft-640_pt-tuluif-7552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: shougan (shougan-university-of-waterloo) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n",
      "wandb: Tracking run with wandb version 0.21.0\n",
      "wandb: Run data is saved locally in /project/6105902/shougan/balance-budget/wandb/run-20260125_203807-9vfd7vvg\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run llama3-8B_llama3-8B_ppl-6.00_sft-640_pt-tuluif-7552\n",
      "wandb: â­ï¸ View project at https://wandb.ai/shougan-university-of-waterloo/tuning\n",
      "wandb: ðŸš€ View run at https://wandb.ai/shougan-university-of-waterloo/tuning/runs/9vfd7vvg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per device train batch size: 1\n",
      "Getting train dataset for run config: llama3-8B_llama3-8B_ppl-6.00_sft-640_pt-tuluif-7552\n",
      "Checking for dataset at /home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/data/datasets/pt-tuluif-7552\n",
      "Dataset already exists at /home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/data/datasets/pt-tuluif-7552\n",
      "Sampled dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'prompt', 'constraints', 'chosen', 'rejected', 'chonsen_model', 'rejected_model', 'system_message'],\n",
      "        num_rows: 7552\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'prompt', 'constraints', 'chosen', 'rejected', 'chonsen_model', 'rejected_model', 'system_message'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "})\n",
      "Example training row: {'id': 'personas_IF_dqxglsux2n8jeu59qktlnesh', 'prompt': 'Name two famous equestrian events that are part of the international jumping circuit, and format your answer by choosing one from these options: lowercase, UPPERCASE, Title Case.', 'constraints': ['punctuation:use no comma', 'format:choose one from options'], 'chosen': 'LONGINES GLOBAL CHAMPIONS TOUR, ROLEX GRAND SLAM OF SHOW JUMPING', 'rejected': '- longines global champions tour\\n- FEI WORLD CUP JUMPING', 'chonsen_model': 'gpt-4o', 'rejected_model': 'gpt-4o', 'system_message': 'You are a helpful assistant who is an expert at responding to prompts by carefully following the given instructions'}\n",
      "Example evaluation row: {'id': 'personas_IF_kvn9ms052ydm1n10j2zs7c90', 'prompt': 'As a high school student with a passion for astronomy, create a list of reliable sources to learn about the solar system. Each source description should contain the keywords \"planet\" and \"orbit\" at least twice. Provide two separate lists: one for online resources and another for books. Each list should contain exactly 5 bullet points.', 'constraints': ['keywords:letter frequency', ' give two responses', 'format:number of bullet lists'], 'chosen': '**Online Resources:**\\n\\n1. **NASA\\'s Solar System Exploration Website**  \\n   This comprehensive online resource offers detailed information about each planet in our solar system and their unique orbits. With interactive features, you can explore how each planet\\'s orbit affects its climate and visibility.\\n\\n2. **The Planetary Society\\'s Guide to the Solar System**  \\n   This guide provides insights into the planets and their orbits, allowing users to understand the dynamic nature of planetary orbits. Learn how each planet\\'s orbit influences its interaction with other celestial bodies.\\n\\n3. **National Geographic\\'s Solar System 101**  \\n   An engaging online article that covers the basics of each planet, including their orbits around the Sun. Discover how the orbit of each planet contributes to phenomena like seasons and eclipses.\\n\\n4. **BBC\\'s Planets and Orbits Webpage**  \\n   This webpage is part of the BBC\\'s educational content, focusing on the planets and their orbits. It explains the intricacies of how each planet maintains its orbit and the historical context of orbit discoveries.\\n\\n5. **Khan Academy\\'s Astronomy Course**  \\n   This free, educational platform offers a series of videos on the solar system, emphasizing the planets and their orbits. Learn about the gravitational forces that govern each planet\\'s orbit and their effects on the solar system\\'s stability.\\n\\n**Books:**\\n\\n1. **\"The Planets: An Illustrated History of Our Solar System\" by Carole Stott**  \\n   This book provides an in-depth look at each planet, with a special focus on their orbits. It explains how the orbit of each planet was discovered and how it affects the solar system.\\n\\n2. **\"The Solar System: An Astronomer\\'s Guide\" by David A. Rothery**  \\n   This guide explores the planets and their orbits, highlighting the science behind orbital mechanics. Learn how each planet\\'s orbit is influenced by gravitational forces.\\n\\n3. **\"Planetary Science: The Science of Planets around Stars\" by George H. A. Cole and Michael M. Woolfson**  \\n   This textbook covers the planets in our solar system and the orbits they follow. It provides a detailed analysis of how each planet\\'s orbit affects its environment and evolution.\\n\\n4. **\"The Orbits of Planets: A Journey Through the Solar System\" by John E. Chambers**  \\n   Focused on the paths that planets take, this book explores the orbits of each planet. It delves into the history and science of how planetary orbits were understood.\\n\\n5. **\"Planets and Life: The Emerging Science of Astrobiology\" by Woodruff T. Sullivan III and John Baross**  \\n   This book examines the planets and their orbits, discussing how orbital patterns can influence the potential for life. Understand how each planet\\'s orbit affects its climate and habitability.', 'rejected': '### Online Resources\\n\\n- **NASA Solar System Exploration**: This official NASA website provides comprehensive information about each planet in our solar system, detailing their orbits, characteristics, and exploration missions. It includes interactive features to visualize the orbits of planets and their positions in the solar system, making it an excellent resource for understanding planetary orbits.\\n\\n- **National Geographic\\'s Solar System 101**: An informative section on National Geographic\\'s website that covers the basics of each planet, their unique features, and how they orbit the Sun. With rich visual content, it helps users grasp the concept of planetary orbits and how each planet\\'s orbit affects its climate and environment.\\n\\n- **European Space Agency (ESA) Solar System and Beyond**: The ESA website offers detailed articles and videos about the planets and their orbits, focusing on the latest scientific discoveries. It explains how the orbits of planets are studied using various space missions and the impact of these orbits on solar system dynamics.\\n\\n- **BBC\\'s The Planets**: This online series explores the planets of our solar system, emphasizing their orbits and how they have evolved over time. It features engaging multimedia content that discusses the unique orbital paths of each planet and how these orbits contribute to their characteristics.\\n\\n- **Space.com Solar System Guide**: Space.com provides articles and infographics about the planets and their orbits, offering a user-friendly approach to learning about the solar system. The guide explains the significance of a planet\\'s orbit, how it is calculated, and its relevance to astronomical studies.\\n\\n### Books\\n\\n- **\"The Planets\" by Dava Sobel**: This book offers a captivating journey through the solar system, exploring each planet and its orbit in detail. It delves into the history of how the orbits of planets were discovered and the impact of these orbits on our understanding of the cosmos.\\n\\n- **\"National Geographic Kids: First Big Book of Space\" by Catherine D. Hughes**: Aimed at younger readers, this book introduces the planets and their orbits with colorful illustrations and simple explanations. It highlights how each planet\\'s orbit affects its position in the solar system and its relationship with other planets.\\n\\n- **\"The Solar System: A Visual Exploration of the Planets, Moons, and Other Heavenly Bodies that Orbit Our Sun\" by Marcus Chown**: This visually stunning book provides in-depth information about the planets and their orbits. It discusses the science behind planetary orbits and how these orbits influence the solar system\\'s structure.\\n\\n- **\"Astronomy for Kids: How to Explore Outer Space with Binoculars, a Telescope, or Just Your Eyes!\" by Bruce Betts**: This educational book encourages young astronomers to observe the planets and their orbits. It explains how to track the orbits of planets using simple tools and understand their movement across the sky.\\n\\n- **\"Planetarium\" by Raman Prinja**: Part of the Welcome to the Museum series, this book offers a detailed look at the planets and their orbits. It includes illustrations and explanations of how the orbits of planets are studied and why these orbits are crucial for understanding the solar system\\'s mechanics.', 'chonsen_model': 'gpt-4o', 'rejected_model': 'gpt-4o', 'system_message': 'You are a helpful assistant who is an expert at responding to prompts by carefully following the given instructions'}\n",
      "Loading model from /home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/models/llama3-8B_ppl-6.00_sft-640\n",
      "==((====))==  Unsloth 2025.10.1: Fast Llama patching. Transformers: 4.56.1. vLLM: 0.9.2+computecanada.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.189 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.66it/s]\n",
      "Unsloth: Will map <|im_end|> to EOS = <|im_end|>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chonsen_model': 'gpt-4o',\n",
      " 'chosen': '<|im_start|>assistant\\n'\n",
      "           'LONGINES GLOBAL CHAMPIONS TOUR, ROLEX GRAND SLAM OF SHOW '\n",
      "           'JUMPING<|im_end|>\\n',\n",
      " 'constraints': ['punctuation:use no comma', 'format:choose one from options'],\n",
      " 'id': 'personas_IF_dqxglsux2n8jeu59qktlnesh',\n",
      " 'prompt': '<|im_start|>system\\n'\n",
      "           'You are a helpful assistant who is an expert at responding to '\n",
      "           'prompts by carefully following the given instructions<|im_end|>\\n'\n",
      "           '<|im_start|>user\\n'\n",
      "           'Name two famous equestrian events that are part of the '\n",
      "           'international jumping circuit, and format your answer by choosing '\n",
      "           'one from these options: lowercase, UPPERCASE, Title '\n",
      "           'Case.<|im_end|>\\n'\n",
      "           '<|im_start|>assistant\\n',\n",
      " 'rejected': '<|im_start|>assistant\\n'\n",
      "             '- longines global champions tour\\n'\n",
      "             '- FEI WORLD CUP JUMPING<|im_end|>\\n',\n",
      " 'rejected_model': 'gpt-4o',\n",
      " 'system_message': 'You are a helpful assistant who is an expert at responding '\n",
      "                   'to prompts by carefully following the given instructions'}\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'prompt', 'constraints', 'chosen', 'rejected', 'chonsen_model', 'rejected_model', 'system_message'],\n",
      "        num_rows: 7552\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'prompt', 'constraints', 'chosen', 'rejected', 'chonsen_model', 'rejected_model', 'system_message'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "})\n",
      "Using LORA with config: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.10.1 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded - <class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "Allocated: 15.30 GB\n",
      "Cached: 15.31 GB\n",
      "[PassAtKCallback] Initialized with pass@1 thresholds=[1.2]\n",
      "[PassAtKCallback] Training will stop at final threshold: 1.2\n",
      "[PassAtKCallback] k_values=[1] (stopping on k=1)\n",
      "[PassAtKCallback] n_samples=1, temperature=0.7, strict=True\n",
      "[PassAtKCallback] IFEval prompts loaded: 541, num_prompts=100\n",
      "[PassAtKCallback] Using vLLM with model save/load (replicating run_inference_ifeval)\n",
      "[DPO] Will stop training when pass@1 >= [1.2]\n",
      "[PerplexityCallback] Initialized with perplexity_thresholds=[0.1]\n",
      "[PerplexityCallback] Training will stop at final threshold: 0.1\n",
      "[PerplexityCallback] num_samples=200\n",
      "[PerplexityCallback] Test dataset size: 200\n",
      "Dataset sample 1st one: {'id': 'personas_IF_kvn9ms052ydm1n10j2zs7c90', 'prompt': '<|im_start|>system\\nYou are a helpful assistant who is an expert at responding to prompts by carefully following the given instructions<|im_end|>\\n<|im_start|>user\\nAs a high school student with a passion for astronomy, create a list of reliable sources to learn about the solar system. Each source description should contain the keywords \"planet\" and \"orbit\" at least twice. Provide two separate lists: one for online resources and another for books. Each list should contain exactly 5 bullet points.<|im_end|>\\n<|im_start|>assistant\\n', 'constraints': ['keywords:letter frequency', ' give two responses', 'format:number of bullet lists'], 'chosen': '<|im_start|>assistant\\n**Online Resources:**\\n\\n1. **NASA\\'s Solar System Exploration Website**  \\n   This comprehensive online resource offers detailed information about each planet in our solar system and their unique orbits. With interactive features, you can explore how each planet\\'s orbit affects its climate and visibility.\\n\\n2. **The Planetary Society\\'s Guide to the Solar System**  \\n   This guide provides insights into the planets and their orbits, allowing users to understand the dynamic nature of planetary orbits. Learn how each planet\\'s orbit influences its interaction with other celestial bodies.\\n\\n3. **National Geographic\\'s Solar System 101**  \\n   An engaging online article that covers the basics of each planet, including their orbits around the Sun. Discover how the orbit of each planet contributes to phenomena like seasons and eclipses.\\n\\n4. **BBC\\'s Planets and Orbits Webpage**  \\n   This webpage is part of the BBC\\'s educational content, focusing on the planets and their orbits. It explains the intricacies of how each planet maintains its orbit and the historical context of orbit discoveries.\\n\\n5. **Khan Academy\\'s Astronomy Course**  \\n   This free, educational platform offers a series of videos on the solar system, emphasizing the planets and their orbits. Learn about the gravitational forces that govern each planet\\'s orbit and their effects on the solar system\\'s stability.\\n\\n**Books:**\\n\\n1. **\"The Planets: An Illustrated History of Our Solar System\" by Carole Stott**  \\n   This book provides an in-depth look at each planet, with a special focus on their orbits. It explains how the orbit of each planet was discovered and how it affects the solar system.\\n\\n2. **\"The Solar System: An Astronomer\\'s Guide\" by David A. Rothery**  \\n   This guide explores the planets and their orbits, highlighting the science behind orbital mechanics. Learn how each planet\\'s orbit is influenced by gravitational forces.\\n\\n3. **\"Planetary Science: The Science of Planets around Stars\" by George H. A. Cole and Michael M. Woolfson**  \\n   This textbook covers the planets in our solar system and the orbits they follow. It provides a detailed analysis of how each planet\\'s orbit affects its environment and evolution.\\n\\n4. **\"The Orbits of Planets: A Journey Through the Solar System\" by John E. Chambers**  \\n   Focused on the paths that planets take, this book explores the orbits of each planet. It delves into the history and science of how planetary orbits were understood.\\n\\n5. **\"Planets and Life: The Emerging Science of Astrobiology\" by Woodruff T. Sullivan III and John Baross**  \\n   This book examines the planets and their orbits, discussing how orbital patterns can influence the potential for life. Understand how each planet\\'s orbit affects its climate and habitability.<|im_end|>\\n', 'rejected': '<|im_start|>assistant\\n### Online Resources\\n\\n- **NASA Solar System Exploration**: This official NASA website provides comprehensive information about each planet in our solar system, detailing their orbits, characteristics, and exploration missions. It includes interactive features to visualize the orbits of planets and their positions in the solar system, making it an excellent resource for understanding planetary orbits.\\n\\n- **National Geographic\\'s Solar System 101**: An informative section on National Geographic\\'s website that covers the basics of each planet, their unique features, and how they orbit the Sun. With rich visual content, it helps users grasp the concept of planetary orbits and how each planet\\'s orbit affects its climate and environment.\\n\\n- **European Space Agency (ESA) Solar System and Beyond**: The ESA website offers detailed articles and videos about the planets and their orbits, focusing on the latest scientific discoveries. It explains how the orbits of planets are studied using various space missions and the impact of these orbits on solar system dynamics.\\n\\n- **BBC\\'s The Planets**: This online series explores the planets of our solar system, emphasizing their orbits and how they have evolved over time. It features engaging multimedia content that discusses the unique orbital paths of each planet and how these orbits contribute to their characteristics.\\n\\n- **Space.com Solar System Guide**: Space.com provides articles and infographics about the planets and their orbits, offering a user-friendly approach to learning about the solar system. The guide explains the significance of a planet\\'s orbit, how it is calculated, and its relevance to astronomical studies.\\n\\n### Books\\n\\n- **\"The Planets\" by Dava Sobel**: This book offers a captivating journey through the solar system, exploring each planet and its orbit in detail. It delves into the history of how the orbits of planets were discovered and the impact of these orbits on our understanding of the cosmos.\\n\\n- **\"National Geographic Kids: First Big Book of Space\" by Catherine D. Hughes**: Aimed at younger readers, this book introduces the planets and their orbits with colorful illustrations and simple explanations. It highlights how each planet\\'s orbit affects its position in the solar system and its relationship with other planets.\\n\\n- **\"The Solar System: A Visual Exploration of the Planets, Moons, and Other Heavenly Bodies that Orbit Our Sun\" by Marcus Chown**: This visually stunning book provides in-depth information about the planets and their orbits. It discusses the science behind planetary orbits and how these orbits influence the solar system\\'s structure.\\n\\n- **\"Astronomy for Kids: How to Explore Outer Space with Binoculars, a Telescope, or Just Your Eyes!\" by Bruce Betts**: This educational book encourages young astronomers to observe the planets and their orbits. It explains how to track the orbits of planets using simple tools and understand their movement across the sky.\\n\\n- **\"Planetarium\" by Raman Prinja**: Part of the Welcome to the Museum series, this book offers a detailed look at the planets and their orbits. It includes illustrations and explanations of how the orbits of planets are studied and why these orbits are crucial for understanding the solar system\\'s mechanics.<|im_end|>\\n', 'chonsen_model': 'gpt-4o', 'rejected_model': 'gpt-4o', 'system_message': 'You are a helpful assistant who is an expert at responding to prompts by carefully following the given instructions'}\n",
      "[DPO] Perplexity thresholds: [0.1]\n",
      "[DPO] Will checkpoint at each threshold and stop at final: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 7,552 | Num Epochs = 2 | Total steps = 944\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 16\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 16 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 83,886,080 of 8,114,147,328 (1.03% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PassAtKCallback] on_train_begin: model_name=PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=14336, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[PerplexityCallback] on_train_begin: model_name=llama3-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/944 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                        | 1/944 [00:06<1:44:30,  6.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6931, 'grad_norm': 5.542614459991455, 'learning_rate': 0.0, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -496.4750061035156, 'logps/rejected': -459.43695068359375, 'logits/chosen': -1.4185739755630493, 'logits/rejected': -1.3989603519439697, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                        | 2/944 [00:11<1:23:23,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6931, 'grad_norm': 4.95107889175415, 'learning_rate': 5.263157894736842e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -411.11932373046875, 'logps/rejected': -421.44915771484375, 'logits/chosen': -1.4528157711029053, 'logits/rejected': -1.4479374885559082, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|â–                                       | 3/944 [00:15<1:17:02,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6943, 'grad_norm': 4.950500011444092, 'learning_rate': 1.0526315789473685e-07, 'rewards/chosen': 0.008936905302107334, 'rewards/rejected': 0.008951233699917793, 'rewards/accuracies': 0.4375, 'rewards/margins': -1.4328397810459137e-05, 'logps/chosen': -321.230712890625, 'logps/rejected': -382.76019287109375, 'logits/chosen': -1.3234643936157227, 'logits/rejected': -1.3355478048324585, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|â–                                       | 4/944 [00:19<1:13:24,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6998, 'grad_norm': 4.5315327644348145, 'learning_rate': 1.5789473684210527e-07, 'rewards/chosen': -0.009216070175170898, 'rewards/rejected': 0.0010401720646768808, 'rewards/accuracies': 0.5625, 'rewards/margins': -0.010256243869662285, 'logps/chosen': -337.52410888671875, 'logps/rejected': -365.7851867675781, 'logits/chosen': -1.3607856035232544, 'logits/rejected': -1.319823980331421, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–                                       | 5/944 [00:24<1:10:40,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7051, 'grad_norm': 5.238068103790283, 'learning_rate': 2.105263157894737e-07, 'rewards/chosen': -0.002538442611694336, 'rewards/rejected': 0.02013874053955078, 'rewards/accuracies': 0.4375, 'rewards/margins': -0.022677181288599968, 'logps/chosen': -404.7428894042969, 'logps/rejected': -443.9204406738281, 'logits/chosen': -1.463504433631897, 'logits/rejected': -1.5112978219985962, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–Ž                                       | 6/944 [00:28<1:09:45,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7128, 'grad_norm': 5.150598049163818, 'learning_rate': 2.6315789473684213e-07, 'rewards/chosen': -0.008800648152828217, 'rewards/rejected': 0.028186824172735214, 'rewards/accuracies': 0.3125, 'rewards/margins': -0.03698747232556343, 'logps/chosen': -385.275390625, 'logps/rejected': -385.5395202636719, 'logits/chosen': -1.4474703073501587, 'logits/rejected': -1.444908857345581, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–Ž                                       | 7/944 [00:32<1:08:36,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6934, 'grad_norm': 5.019770622253418, 'learning_rate': 3.1578947368421055e-07, 'rewards/chosen': 0.01588423177599907, 'rewards/rejected': 0.014317880384624004, 'rewards/accuracies': 0.4375, 'rewards/margins': 0.0015663495287299156, 'logps/chosen': -326.447509765625, 'logps/rejected': -365.7529602050781, 'logits/chosen': -1.3292007446289062, 'logits/rejected': -1.3167366981506348, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–Ž                                       | 8/944 [00:36<1:07:22,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7008, 'grad_norm': 4.357771396636963, 'learning_rate': 3.6842105263157896e-07, 'rewards/chosen': -0.009958887472748756, 'rewards/rejected': 0.0023901103995740414, 'rewards/accuracies': 0.5, 'rewards/margins': -0.012349000200629234, 'logps/chosen': -323.2181396484375, 'logps/rejected': -334.09075927734375, 'logits/chosen': -1.2611509561538696, 'logits/rejected': -1.2760438919067383, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–                                       | 9/944 [00:40<1:06:28,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6703, 'grad_norm': 4.660864353179932, 'learning_rate': 4.210526315789474e-07, 'rewards/chosen': 0.03171060234308243, 'rewards/rejected': -0.017680097371339798, 'rewards/accuracies': 0.75, 'rewards/margins': 0.04939069598913193, 'logps/chosen': -339.4760437011719, 'logps/rejected': -379.665771484375, 'logits/chosen': -1.4812134504318237, 'logits/rejected': -1.3970197439193726, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–                                      | 10/944 [00:45<1:06:38,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7084, 'grad_norm': 6.092506408691406, 'learning_rate': 4.7368421052631585e-07, 'rewards/chosen': 0.004922008141875267, 'rewards/rejected': 0.03290505334734917, 'rewards/accuracies': 0.5, 'rewards/margins': -0.02798304706811905, 'logps/chosen': -421.2081604003906, 'logps/rejected': -377.359619140625, 'logits/chosen': -1.4381415843963623, 'logits/rejected': -1.4652718305587769, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–                                      | 11/944 [00:49<1:06:11,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6971, 'grad_norm': 4.8614654541015625, 'learning_rate': 5.263157894736843e-07, 'rewards/chosen': -0.0060197836719453335, 'rewards/rejected': -0.00014584045857191086, 'rewards/accuracies': 0.5625, 'rewards/margins': -0.00587394367903471, 'logps/chosen': -377.4822692871094, 'logps/rejected': -348.406005859375, 'logits/chosen': -1.455124855041504, 'logits/rejected': -1.4418792724609375, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–                                      | 12/944 [00:53<1:06:06,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6933, 'grad_norm': 5.4443488121032715, 'learning_rate': 5.789473684210526e-07, 'rewards/chosen': -0.012450600042939186, 'rewards/rejected': -0.0163358673453331, 'rewards/accuracies': 0.625, 'rewards/margins': 0.003885268699377775, 'logps/chosen': -309.88623046875, 'logps/rejected': -432.39935302734375, 'logits/chosen': -1.3633791208267212, 'logits/rejected': -1.4484801292419434, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–Œ                                      | 13/944 [00:57<1:06:10,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6932, 'grad_norm': 4.746312141418457, 'learning_rate': 6.315789473684211e-07, 'rewards/chosen': 0.007894826121628284, 'rewards/rejected': 0.005517030134797096, 'rewards/accuracies': 0.5, 'rewards/margins': 0.002377795986831188, 'logps/chosen': -378.1329345703125, 'logps/rejected': -391.86993408203125, 'logits/chosen': -1.385936975479126, 'logits/rejected': -1.4664204120635986, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–Œ                                      | 14/944 [01:02<1:04:55,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7038, 'grad_norm': 3.666637420654297, 'learning_rate': 6.842105263157896e-07, 'rewards/chosen': -0.016512561589479446, 'rewards/rejected': 0.0038567068986594677, 'rewards/accuracies': 0.4375, 'rewards/margins': -0.0203692689538002, 'logps/chosen': -239.3567657470703, 'logps/rejected': -260.0445251464844, 'logits/chosen': -1.423860788345337, 'logits/rejected': -1.3871805667877197, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–Œ                                      | 15/944 [01:06<1:05:38,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6802, 'grad_norm': 5.892290115356445, 'learning_rate': 7.368421052631579e-07, 'rewards/chosen': 0.02724769338965416, 'rewards/rejected': -0.0009139061439782381, 'rewards/accuracies': 0.5625, 'rewards/margins': 0.02816159650683403, 'logps/chosen': -490.51458740234375, 'logps/rejected': -488.53741455078125, 'logits/chosen': -1.46258544921875, 'logits/rejected': -1.4764724969863892, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–‹                                      | 16/944 [01:10<1:05:27,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6869, 'grad_norm': 4.9815754890441895, 'learning_rate': 7.894736842105263e-07, 'rewards/chosen': -0.0166414026170969, 'rewards/rejected': -0.031662680208683014, 'rewards/accuracies': 0.4375, 'rewards/margins': 0.015021274797618389, 'logps/chosen': -378.0012512207031, 'logps/rejected': -380.3297119140625, 'logits/chosen': -1.2848806381225586, 'logits/rejected': -1.2836599349975586, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–‹                                      | 17/944 [01:14<1:05:01,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6878, 'grad_norm': 4.207923889160156, 'learning_rate': 8.421052631578948e-07, 'rewards/chosen': -0.0023411866277456284, 'rewards/rejected': -0.013772642239928246, 'rewards/accuracies': 0.5625, 'rewards/margins': 0.011431455612182617, 'logps/chosen': -355.66943359375, 'logps/rejected': -350.8884582519531, 'logits/chosen': -1.3793643712997437, 'logits/rejected': -1.4109342098236084, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–‹                                      | 18/944 [01:19<1:05:23,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7064, 'grad_norm': 4.864419937133789, 'learning_rate': 8.947368421052632e-07, 'rewards/chosen': 0.021042052656412125, 'rewards/rejected': 0.044378019869327545, 'rewards/accuracies': 0.4375, 'rewards/margins': -0.02333596907556057, 'logps/chosen': -399.75860595703125, 'logps/rejected': -391.28314208984375, 'logits/chosen': -1.357136845588684, 'logits/rejected': -1.3863729238510132, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–Š                                      | 19/944 [01:23<1:04:57,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6855, 'grad_norm': 4.812286376953125, 'learning_rate': 9.473684210526317e-07, 'rewards/chosen': 0.008062982931733131, 'rewards/rejected': -0.008312511257827282, 'rewards/accuracies': 0.5625, 'rewards/margins': 0.016375495120882988, 'logps/chosen': -409.21832275390625, 'logps/rejected': -462.539794921875, 'logits/chosen': -1.4573144912719727, 'logits/rejected': -1.4960923194885254, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–Š                                      | 20/944 [01:27<1:05:18,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7291, 'grad_norm': 5.29579496383667, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': -0.060369592159986496, 'rewards/rejected': 0.008015083149075508, 'rewards/accuracies': 0.25, 'rewards/margins': -0.06838466972112656, 'logps/chosen': -416.2986145019531, 'logps/rejected': -408.3861083984375, 'logits/chosen': -1.468083143234253, 'logits/rejected': -1.5050705671310425, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–Š                                      | 21/944 [01:31<1:04:11,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6852, 'grad_norm': 5.1438469886779785, 'learning_rate': 1.0526315789473685e-06, 'rewards/chosen': 0.012639069929718971, 'rewards/rejected': -0.005601857788860798, 'rewards/accuracies': 0.5625, 'rewards/margins': 0.018240928649902344, 'logps/chosen': -200.19412231445312, 'logps/rejected': -276.3273620605469, 'logits/chosen': -1.3313597440719604, 'logits/rejected': -1.3713085651397705, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–‰                                      | 22/944 [01:35<1:03:38,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6982, 'grad_norm': 4.496091842651367, 'learning_rate': 1.1052631578947369e-06, 'rewards/chosen': 0.022543001919984818, 'rewards/rejected': 0.03141884505748749, 'rewards/accuracies': 0.5, 'rewards/margins': -0.008875847794115543, 'logps/chosen': -329.7518310546875, 'logps/rejected': -350.4999084472656, 'logits/chosen': -1.449445366859436, 'logits/rejected': -1.5042977333068848, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–‰                                      | 23/944 [01:39<1:03:20,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6944, 'grad_norm': 4.090702533721924, 'learning_rate': 1.1578947368421053e-06, 'rewards/chosen': 0.025652490556240082, 'rewards/rejected': 0.027488086372613907, 'rewards/accuracies': 0.5625, 'rewards/margins': -0.0018355953507125378, 'logps/chosen': -241.3975372314453, 'logps/rejected': -284.32183837890625, 'logits/chosen': -1.2237783670425415, 'logits/rejected': -1.2904306650161743, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–‰                                      | 24/944 [01:43<1:02:53,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.685, 'grad_norm': 4.733790874481201, 'learning_rate': 1.2105263157894738e-06, 'rewards/chosen': 0.022380303591489792, 'rewards/rejected': 0.005219268146902323, 'rewards/accuracies': 0.6875, 'rewards/margins': 0.017161035910248756, 'logps/chosen': -257.5514221191406, 'logps/rejected': -338.91497802734375, 'logits/chosen': -1.3120601177215576, 'logits/rejected': -1.3376638889312744, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–ˆ                                      | 25/944 [01:47<1:03:19,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7247, 'grad_norm': 4.596199035644531, 'learning_rate': 1.2631578947368422e-06, 'rewards/chosen': -0.006106340792030096, 'rewards/rejected': 0.0535212904214859, 'rewards/accuracies': 0.25, 'rewards/margins': -0.059627629816532135, 'logps/chosen': -300.976318359375, 'logps/rejected': -306.94573974609375, 'logits/chosen': -1.204613208770752, 'logits/rejected': -1.1544411182403564, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–ˆ                                      | 26/944 [01:51<1:02:54,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6856, 'grad_norm': 4.8678765296936035, 'learning_rate': 1.3157894736842106e-06, 'rewards/chosen': 0.0036851526238024235, 'rewards/rejected': -0.01249399222433567, 'rewards/accuracies': 0.5625, 'rewards/margins': 0.016179142519831657, 'logps/chosen': -323.5550842285156, 'logps/rejected': -322.27337646484375, 'logits/chosen': -1.4284437894821167, 'logits/rejected': -1.4872446060180664, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–ˆ                                      | 27/944 [01:56<1:03:01,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.71, 'grad_norm': 4.274747848510742, 'learning_rate': 1.3684210526315791e-06, 'rewards/chosen': -0.01651310920715332, 'rewards/rejected': 0.01589488983154297, 'rewards/accuracies': 0.5, 'rewards/margins': -0.03240799903869629, 'logps/chosen': -345.21435546875, 'logps/rejected': -334.0679931640625, 'logits/chosen': -1.455175518989563, 'logits/rejected': -1.4684306383132935, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–ˆâ–                                     | 28/944 [02:00<1:02:53,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6867, 'grad_norm': 4.559662342071533, 'learning_rate': 1.4210526315789475e-06, 'rewards/chosen': 0.009488438256084919, 'rewards/rejected': -0.005594301037490368, 'rewards/accuracies': 0.375, 'rewards/margins': 0.015082740224897861, 'logps/chosen': -346.5003356933594, 'logps/rejected': -385.8034362792969, 'logits/chosen': -1.4427299499511719, 'logits/rejected': -1.5449353456497192, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–ˆâ–                                     | 29/944 [02:04<1:03:21,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7015, 'grad_norm': 5.281733512878418, 'learning_rate': 1.4736842105263159e-06, 'rewards/chosen': -0.00867452658712864, 'rewards/rejected': 0.005874251946806908, 'rewards/accuracies': 0.5, 'rewards/margins': -0.014548779465258121, 'logps/chosen': -404.7667541503906, 'logps/rejected': -467.3251953125, 'logits/chosen': -1.5043227672576904, 'logits/rejected': -1.503151297569275, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–ˆâ–                                     | 30/944 [02:08<1:02:58,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6861, 'grad_norm': 4.937984943389893, 'learning_rate': 1.5263157894736844e-06, 'rewards/chosen': 0.01058135088533163, 'rewards/rejected': -0.004786182194948196, 'rewards/accuracies': 0.6875, 'rewards/margins': 0.015367534011602402, 'logps/chosen': -298.0951843261719, 'logps/rejected': -360.6861877441406, 'logits/chosen': -1.3176229000091553, 'logits/rejected': -1.2951431274414062, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–ˆâ–Ž                                     | 31/944 [02:12<1:02:15,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6939, 'grad_norm': 4.150136470794678, 'learning_rate': 1.5789473684210526e-06, 'rewards/chosen': -0.005039251875132322, 'rewards/rejected': -0.004492616280913353, 'rewards/accuracies': 0.5, 'rewards/margins': -0.0005466346628963947, 'logps/chosen': -300.4503479003906, 'logps/rejected': -281.1486511230469, 'logits/chosen': -1.5413607358932495, 'logits/rejected': -1.474329948425293, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–ˆâ–Ž                                     | 32/944 [02:16<1:02:27,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7044, 'grad_norm': 4.804490089416504, 'learning_rate': 1.6315789473684212e-06, 'rewards/chosen': 0.004730460233986378, 'rewards/rejected': 0.025455977767705917, 'rewards/accuracies': 0.3125, 'rewards/margins': -0.020725512877106667, 'logps/chosen': -336.5945129394531, 'logps/rejected': -342.72509765625, 'logits/chosen': -1.467255711555481, 'logits/rejected': -1.499653697013855, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–ˆâ–Ž                                     | 33/944 [02:21<1:03:14,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6921, 'grad_norm': 5.6562323570251465, 'learning_rate': 1.6842105263157895e-06, 'rewards/chosen': 0.005371606443077326, 'rewards/rejected': 0.0019052503630518913, 'rewards/accuracies': 0.5625, 'rewards/margins': 0.003466355614364147, 'logps/chosen': -430.41070556640625, 'logps/rejected': -436.98052978515625, 'logits/chosen': -1.4362167119979858, 'logits/rejected': -1.4271538257598877, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–ˆâ–                                     | 34/944 [02:25<1:02:28,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6913, 'grad_norm': 4.572940349578857, 'learning_rate': 1.736842105263158e-06, 'rewards/chosen': -0.0017172335647046566, 'rewards/rejected': -0.0059153544716537, 'rewards/accuracies': 0.4375, 'rewards/margins': 0.004198120906949043, 'logps/chosen': -309.0335388183594, 'logps/rejected': -294.1842346191406, 'logits/chosen': -1.461841106414795, 'logits/rejected': -1.497746229171753, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–ˆâ–                                     | 35/944 [02:29<1:01:54,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6971, 'grad_norm': 5.176706790924072, 'learning_rate': 1.7894736842105265e-06, 'rewards/chosen': 0.003620815696194768, 'rewards/rejected': 0.010051597841084003, 'rewards/accuracies': 0.4375, 'rewards/margins': -0.006430780049413443, 'logps/chosen': -290.6890869140625, 'logps/rejected': -293.8875427246094, 'logits/chosen': -1.4535040855407715, 'logits/rejected': -1.4691929817199707, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–ˆâ–                                     | 36/944 [02:33<1:02:04,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.697, 'grad_norm': 4.758002758026123, 'learning_rate': 1.8421052631578948e-06, 'rewards/chosen': -0.022864796221256256, 'rewards/rejected': -0.017035674303770065, 'rewards/accuracies': 0.5625, 'rewards/margins': -0.005829118192195892, 'logps/chosen': -398.71502685546875, 'logps/rejected': -402.7589111328125, 'logits/chosen': -1.4137160778045654, 'logits/rejected': -1.4474347829818726, 'epoch': 0.08}\n"
     ]
    }
   ],
   "source": [
    "for checkpoint in checkpoints:\n",
    "    model_name = Path(checkpoint[\"checkpoint_path\"]).name\n",
    "    data = total_train_size - checkpoint[\"data_points_seen\"] \n",
    "    subprocess.run([\"bash\", \"tuning/slurm/run_dpo.sh\", \"llama3-8B\", \"tuluif\", str(data), \"ifeval\", \"dpo\", model_name, str(checkpoint[\"data_points_seen\"]) ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f688917d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
