{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41f31555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 02-06 12:15:35 [__init__.py:244] Automatically detected platform cuda.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shougan/projects/aip-fredashi/shougan/balance-budget/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/home/shougan/projects/aip-fredashi/shougan/balance-budget/venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning\n"
     ]
    }
   ],
   "source": [
    "from tuning.utils.utils import chat_template_func\n",
    "from vllm import LLM, SamplingParams\n",
    "from tuning.config import MODELS_DIR\n",
    "from tuning.inference.config_inference import VLLMSamplingParamsConfig\n",
    "from tuning.data.test_dataset import get_ifeval_test_dataset\n",
    "import os\n",
    "import wandb\n",
    "wandb.disabled = True\n",
    "os.environ['WANDB_DISABLED'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b58cbd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-06 12:15:56 [config.py:841] This model supports multiple tasks: {'classify', 'generate', 'reward', 'embed'}. Defaulting to 'generate'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-06 12:15:56 [config.py:3368] Downcasting torch.float32 to torch.bfloat16.\n",
      "INFO 02-06 12:15:56 [config.py:1472] Using max model len 131072\n",
      "INFO 02-06 12:15:59 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 02-06 12:15:59 [__init__.py:2662] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 02-06 12:16:06 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 02-06 12:16:08 [core.py:526] Waiting for init message from front-end.\n",
      "INFO 02-06 12:16:08 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/models/llama3-1B', speculative_config=None, tokenizer='/home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/models/llama3-1B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/models/llama3-1B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 02-06 12:16:08 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 02-06 12:16:08 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 02-06 12:16:08 [gpu_model_runner.py:1770] Starting to load model /home/shougan/projects/aip-fredashi/shougan/balance-budget/tuning/models/llama3-1B...\n",
      "INFO 02-06 12:16:09 [gpu_model_runner.py:1775] Loading model from scratch...\n",
      "INFO 02-06 12:16:09 [cuda.py:284] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.92s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.92s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-06 12:16:11 [default_loader.py:272] Loading weights took 2.00 seconds\n",
      "INFO 02-06 12:16:11 [gpu_model_runner.py:1801] Model loading took 2.3205 GiB and 2.460022 seconds\n",
      "INFO 02-06 12:16:19 [backends.py:508] Using cache directory: /home/shougan/.cache/vllm/torch_compile_cache/3019068c4d/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 02-06 12:16:19 [backends.py:519] Dynamo bytecode transform time: 7.13 s\n",
      "INFO 02-06 12:16:22 [backends.py:155] Directly load the compiled graph(s) for shape None from the cache, took 2.659 s\n",
      "INFO 02-06 12:16:22 [monitor.py:34] torch.compile takes 7.13 s in total\n",
      "INFO 02-06 12:16:23 [gpu_worker.py:232] Available KV cache memory: 64.03 GiB\n",
      "INFO 02-06 12:16:23 [kv_cache_utils.py:716] GPU KV cache size: 2,097,984 tokens\n",
      "INFO 02-06 12:16:23 [kv_cache_utils.py:720] Maximum concurrency for 131,072 tokens per request: 16.01x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:17<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-06 12:16:41 [gpu_model_runner.py:2326] Graph capturing finished in 17 secs, took 0.43 GiB\n",
      "INFO 02-06 12:16:41 [core.py:172] init engine (profile, create kv cache, warmup model) took 29.56 seconds\n"
     ]
    }
   ],
   "source": [
    "model_name = \"llama3-1B\"\n",
    "model_path = os.path.join(MODELS_DIR, model_name)\n",
    "llm = LLM(model=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "600c272b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.5, top_p=0.9, top_k=150, min_p=0.0, seed=None, stop=['<|im_end|>', '<|end_of_text|>'], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)\n"
     ]
    }
   ],
   "source": [
    "config = VLLMSamplingParamsConfig()\n",
    "config.n = 1\n",
    "sampling_params = SamplingParams(**config.model_dump())\n",
    "print(sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b20779d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|>', '<|im_end|>', '<', '|', 'im', '_start', '|', '>', 'system', 'ÄŠ', 'You', 'Ä are', 'Ä a', 'Ä helpful', 'Ä assistant', 'Ä who', 'Ä is', 'Ä an', 'Ä expert', 'Ä at', 'Ä responding', 'Ä to', 'Ä prompts', 'Ä by', 'Ä carefully', 'Ä following', 'Ä the', 'Ä given', 'Ä instructions', '<|im_end|>', 'ÄŠ', '<', '|', 'im', '_start', '|', '>', 'user', 'ÄŠ', 'Write', 'Ä a', 'Ä ', '300', '+', 'Ä word', 'Ä summary', 'Ä of', 'Ä the', 'Ä wikipedia', 'Ä page', 'Ä \"', 'https', '://', 'en', '.wikipedia', '.org', '/wiki', '/R', 'ay', 'mond', '_', 'III', ',_', 'Count', '_of', '_T', 'rip', 'oli', '\".', 'Ä Do', 'Ä not', 'Ä use', 'Ä any', 'Ä commas', 'Ä and', 'Ä highlight', 'Ä at', 'Ä least', 'Ä ', '3', 'Ä sections', 'Ä that', 'Ä has', 'Ä titles', 'Ä in', 'Ä markdown', 'Ä format', ',', 'Ä for', 'Ä example', 'Ä *', 'highlight', 'ed', 'Ä section', 'Ä part', 'Ä ', '1', '*,', 'Ä *', 'highlight', 'ed', 'Ä section', 'Ä part', 'Ä ', '2', '*,', 'Ä *', 'highlight', 'ed', 'Ä section', 'Ä part', 'Ä ', '3', '*.', '<|im_end|>', 'ÄŠ', '<', '|', 'im', '_start', '|', '>', 'assistant', 'ÄŠ']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = llm.get_tokenizer()\n",
    "tokenizer = chat_template_func(tokenizer)   \n",
    "chat_template = tokenizer.chat_template\n",
    "# Convert the whole list at once\n",
    "ids_to_check = [128000, 27, 91, 318, 5011, 91, 29, 9125, 198, 2675, 527, 264, 11190, 18328, 889, 374, 459, 6335, 520, 30438, 311, 52032, 555, 15884, 2768, 279, 2728, 11470, 128001, 198, 27, 91, 318, 5011, 91, 29, 882, 198, 8144, 264, 220, 3101, 10, 3492, 12399, 315, 279, 59318, 2199, 330, 2485, 1129, 268, 34466, 2726, 26583, 19945, 352, 12669, 62, 23440, 21025, 2568, 3659, 1159, 4664, 14559, 3343, 3234, 539, 1005, 904, 77702, 323, 11415, 520, 3325, 220, 18, 14491, 430, 706, 15671, 304, 51594, 3645, 11, 369, 3187, 353, 36298, 291, 3857, 961, 220, 16, 12594, 353, 36298, 291, 3857, 961, 220, 17, 12594, 353, 36298, 291, 3857, 961, 220, 18, 20517, 128001, 198, 27, 91, 318, 5011, 91, 29, 78191, 198]\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids_to_check)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "990adf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>: 128000\n",
      "<|start_header_id|>: 128006\n",
      "<|eot_id|>: 128009\n",
      "<|end_of_text|>: 128256\n",
      "<|im_end|>: 128001\n",
      "<|im_start|>: None\n"
     ]
    }
   ],
   "source": [
    "special_tokens = [\"<|begin_of_text|>\", \"<|start_header_id|>\", \"<|eot_id|>\", \"<|end_of_text|>\", \"<|im_end|>\", '<|im_start|>']\n",
    "for token in special_tokens:\n",
    "    print(f\"{token}: {tokenizer.convert_tokens_to_ids(token)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "833885e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_ifeval_test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ca22d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c65dc6a45afa4cc2888af12117960ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a2512035e44c10b422b276ea9ef510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/21 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m outputs = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Just run on first 10 samples for demo\u001b[39;49;00m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/aip-fredashi/shougan/balance-budget/venv/lib/python3.11/site-packages/vllm/entrypoints/llm.py:865\u001b[39m, in \u001b[36mLLM.chat\u001b[39m\u001b[34m(self, messages, sampling_params, use_tqdm, lora_request, chat_template, chat_template_content_format, add_generation_prompt, continue_final_message, tools, chat_template_kwargs, mm_processor_kwargs)\u001b[39m\n\u001b[32m    861\u001b[39m         prompt[\u001b[33m\"\u001b[39m\u001b[33mmm_processor_kwargs\u001b[39m\u001b[33m\"\u001b[39m] = mm_processor_kwargs\n\u001b[32m    863\u001b[39m     prompts.append(prompt)\n\u001b[32m--> \u001b[39m\u001b[32m865\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/aip-fredashi/shougan/balance-budget/venv/lib/python3.11/site-packages/vllm/utils/__init__.py:1292\u001b[39m, in \u001b[36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1285\u001b[39m             msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1287\u001b[39m         warnings.warn(\n\u001b[32m   1288\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[32m   1289\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[32m   1290\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1292\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/aip-fredashi/shougan/balance-budget/venv/lib/python3.11/site-packages/vllm/entrypoints/llm.py:510\u001b[39m, in \u001b[36mLLM.generate\u001b[39m\u001b[34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, prompt_adapter_request, guided_options_request, priority)\u001b[39m\n\u001b[32m    496\u001b[39m _validate_truncation_size(\u001b[38;5;28mself\u001b[39m.llm_engine.model_config.max_model_len,\n\u001b[32m    497\u001b[39m                           truncate_prompt_tokens, tokenization_kwargs)\n\u001b[32m    499\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_and_add_requests(\n\u001b[32m    500\u001b[39m     prompts=parsed_prompts,\n\u001b[32m    501\u001b[39m     params=sampling_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m    507\u001b[39m     priority=priority,\n\u001b[32m    508\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m510\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine_class.validate_outputs(outputs, RequestOutput)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/aip-fredashi/shougan/balance-budget/venv/lib/python3.11/site-packages/vllm/entrypoints/llm.py:1570\u001b[39m, in \u001b[36mLLM._run_engine\u001b[39m\u001b[34m(self, use_tqdm)\u001b[39m\n\u001b[32m   1568\u001b[39m total_out_toks = \u001b[32m0\u001b[39m\n\u001b[32m   1569\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.llm_engine.has_unfinished_requests():\n\u001b[32m-> \u001b[39m\u001b[32m1570\u001b[39m     step_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1571\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[32m   1572\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m output.finished:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/aip-fredashi/shougan/balance-budget/venv/lib/python3.11/site-packages/vllm/v1/engine/llm_engine.py:237\u001b[39m, in \u001b[36mLLMEngine.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[32m    236\u001b[39m \u001b[38;5;66;03m# 1) Get EngineCoreOutput from the EngineCore.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine_core\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;66;03m# 2) Process EngineCoreOutputs.\u001b[39;00m\n\u001b[32m    240\u001b[39m iteration_stats = IterationStats() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.log_stats \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/aip-fredashi/shougan/balance-budget/venv/lib/python3.11/site-packages/vllm/v1/engine/core_client.py:569\u001b[39m, in \u001b[36mSyncMPClient.get_output\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    565\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_output\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> EngineCoreOutputs:\n\u001b[32m    566\u001b[39m     \u001b[38;5;66;03m# If an exception arises in process_outputs_socket task,\u001b[39;00m\n\u001b[32m    567\u001b[39m     \u001b[38;5;66;03m# it is forwarded to the outputs_queue so we can raise it\u001b[39;00m\n\u001b[32m    568\u001b[39m     \u001b[38;5;66;03m# from this (run_output_handler) task to shut down the server.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m569\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutputs_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    570\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[32m    571\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_exception(outputs) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/lib/python3.11/queue.py:171\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._qsize():\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m timeout < \u001b[32m0\u001b[39m:\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m'\u001b[39m\u001b[33m must be a non-negative number\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/lib/python3.11/threading.py:320\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    322\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "outputs = llm.chat(\n",
    "    dataset[\"messages\"][:5],  # Just run on first 10 samples for demo\n",
    "    sampling_params, \n",
    "    chat_template=chat_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a560c9cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
