{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "sys.path.insert(0, '/home/shougan/projects/aip-fredashi/shougan/balance-budget')\n",
    "BASE_DIR = Path('/home/shougan/projects/aip-fredashi/shougan/balance-budget')\n",
    "\n",
    "from tuning.config import OUTPUTS_DIR\n",
    "from tuning.inference.ifeval_inference import run_inference_ifeval\n",
    "from instruction_following_eval import evaluation_lib, instructions_registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === UTILITY FUNCTIONS ===\n",
    "\n",
    "def pass_at_k(n: int, c: int, k: int) -> float:\n",
    "    \"\"\"Calculate pass@k: probability that at least one of k samples is correct.\"\"\"\n",
    "    if n - c < k:\n",
    "        return 1.0\n",
    "    return 1.0 - np.prod(1.0 - k / np.arange(n - c + 1, n + 1))\n",
    "\n",
    "def compute_pass_at_k_scores(results_per_prompt: List[List[bool]], k_values: List[int]) -> Dict[int, float]:\n",
    "    \"\"\"Compute average pass@k across all prompts.\"\"\"\n",
    "    scores = {k: [] for k in k_values}\n",
    "    for results in results_per_prompt:\n",
    "        n, c = len(results), sum(results)\n",
    "        for k in k_values:\n",
    "            if k <= n:\n",
    "                scores[k].append(pass_at_k(n, c, k))\n",
    "    return {k: np.mean(v) for k, v in scores.items() if v}\n",
    "\n",
    "def save_responses(results: List[Dict], model_name: str):\n",
    "    path = OUTPUTS / model_name\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path / \"responses_multi_sample.jsonl\", \"w\") as f:\n",
    "        for r in results:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def load_responses(model_name: str) -> List[Dict]:\n",
    "    with open(OUTPUTS / model_name / \"responses_multi_sample.jsonl\") as f:\n",
    "        return [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "model_name = \"llama3-1B\"\n",
    "k_values = [1, 2]\n",
    "n_samples = 4\n",
    "temperature = 0.7\n",
    "run_inference_flag = True\n",
    "num_examples = 20  # Set to None for full dataset\n",
    "\n",
    "# Setup paths\n",
    "OUTPUTS = Path(OUTPUTS_DIR) / \"pass@k_responses\"\n",
    "IFEVAL_INPUT_PATH = BASE_DIR / \"instruction_following_eval/data/input_data.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 1: LOAD IFEVAL INPUTS ===\n",
    "inputs_map = {inp.prompt: inp for inp in evaluation_lib.read_prompt_list(str(IFEVAL_INPUT_PATH))}\n",
    "print(f\"Loaded {len(inputs_map)} IFEval prompts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluation_lib.read_prompt_list(str(IFEVAL_INPUT_PATH))[0])\n",
    "test_prompt = evaluation_lib.read_prompt_list(str(IFEVAL_INPUT_PATH))[0].prompt\n",
    "print(\"\\n\", test_prompt)\n",
    "print(\"\\n\",inputs_map[test_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 2: RUN INFERENCE OR LOAD CACHED RESPONSES ===\n",
    "print(f\"Model: {model_name}\")\n",
    "\n",
    "if run_inference_flag:\n",
    "    print(f\"Running inference with n_samples={n_samples}, temperature={temperature}\")\n",
    "    raw_results = run_inference_ifeval(\n",
    "        model_name=model_name,\n",
    "        n_samples=n_samples,\n",
    "        temperature=temperature,\n",
    "        save_results=False,\n",
    "        num_examples=num_examples\n",
    "    )\n",
    "    # Group responses by prompt for pass@k evaluation\n",
    "    # raw_results: [{prompt: \"\", responses: [\"\", \"\", ...]}, ...] where each response is separate\n",
    "    grouped = defaultdict(list)\n",
    "    for r in raw_results:\n",
    "        grouped[r[\"prompt\"]].append(r[\"response\"])\n",
    "    \n",
    "    model_results = [{\"prompt\": p, \"responses\": resps} for p, resps in grouped.items()]\n",
    "    print(f\"Generated {len(model_results)} prompts with {n_samples} samples each\")\n",
    "    save_responses(model_results, model_name)\n",
    "    print(\"Responses saved.\")\n",
    "else:\n",
    "    print(\"Loading cached responses...\")\n",
    "    model_results = load_responses(model_name)\n",
    "    print(f\"Loaded {len(model_results)} results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 4: EVALUATE SINGLE RESPONSE USING PRE-BUILT FUNCTIONS ===\n",
    "def evaluate_single_response(inp: evaluation_lib.InputExample, response: str, strict: bool = True) -> bool:\n",
    "    \"\"\"Evaluate a single response using the pre-built IFEval functions.\"\"\"\n",
    "    prompt_to_response = {inp.prompt: response}\n",
    "    \n",
    "    if strict:\n",
    "        result = evaluation_lib.test_instruction_following_strict(inp, prompt_to_response)\n",
    "    else:\n",
    "        result = evaluation_lib.test_instruction_following_loose(inp, prompt_to_response)\n",
    "    \n",
    "    return result.follow_all_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_response = model_results[0][\"responses\"][0]\n",
    "test_eval_input = inputs_map[model_results[0][\"prompt\"]]\n",
    "evaluate_single_response(test_eval_input,test_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 5: COMPUTE PASS@K SCORES ===\n",
    "\n",
    "model_name = \"llama3-8B_sft-tuluif-10000\"\n",
    "model_results = load_responses(model_name)\n",
    "final_results_strict = defaultdict(list)\n",
    "final_results_loose = defaultdict(list)\n",
    "# === PIPELINE ===\n",
    "results_per_prompt = {}\n",
    "for item in tqdm(model_results, desc=\"Evaluating responses\"):\n",
    "    prompt = item[\"prompt\"]\n",
    "    responses = item[\"responses\"]\n",
    "    eval_input = inputs_map[prompt]\n",
    "    strict_results = [evaluate_single_response(eval_input, r, strict=True) for r in responses]\n",
    "    loose_results = [evaluate_single_response(eval_input, r, strict=False) for r in responses]\n",
    "    results_per_prompt[prompt] = {\n",
    "        \"strict\": strict_results,\n",
    "        \"loose\": loose_results,\n",
    "        **{f\"strict_k{i}\": pass_at_k(len(strict_results), sum(strict_results), i) for i in k_values},\n",
    "        **{f\"loose_k{i}\": pass_at_k(len(loose_results), sum(loose_results), i) for i in k_values},\n",
    "        \"strict_k\":[pass_at_k(len(strict_results), sum(strict_results), i) for i in k_values],\n",
    "        \"loose_k\":[ pass_at_k(len(loose_results), sum(loose_results), i) for i in k_values]\n",
    "    }\n",
    "    for i in k_values:\n",
    "        final_results_strict[f\"k{i}\"].append(pass_at_k(len(strict_results), sum(strict_results), i))\n",
    "        final_results_loose[f\"k{i}\"].append(pass_at_k(len(loose_results), sum(loose_results), i))\n",
    "    # for r in responses:\n",
    "    #     if evaluate_single_response(eval_input, r, strict=True):\n",
    "    #         print(prompt)\n",
    "    #         print(r)\n",
    "    #         print(\"=\"*50)\n",
    "    #         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print([np.mean(final_results_strict[f\"k{i}\"]) for i in k_values])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sum = 0\n",
    "\n",
    "for prompt, result in results_per_prompt.items():\n",
    "    my_sum += result['loose_k64']\n",
    "print(my_sum/541)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FINAL RESULTS ===\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"RESULTS: {model_name}\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Strict pass@k: {strict_scores}\")\n",
    "print(f\"Loose pass@k:  {loose_scores}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
